{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np \n",
    "import sub_func_ce\n",
    "from sub_func_ce import abs_dis, find_overlap\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, permutations, product\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "# from cuml.model_selection import train_test_split\n",
    "# from cuml.preprocessing import StandardScaler\n",
    "from cuml.linear_model import Lasso\n",
    "from cuml.experimental.linear_model import Lars\n",
    "from cuml.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso as Lasso_sk\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import cupy as cp\n",
    "\n",
    "from random import randrange\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "from msad_ana import msad_ana_qua\n",
    "\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "def normal_dist(x, mu, std):\n",
    "    prob_density = (np.pi*std) * np.exp(-0.5*((x-mu)/std)**2)\n",
    "    return prob_density\n",
    "    \n",
    "def draw_3d(ind_raw):\n",
    "    ind_raw = np.array(ind_raw)\n",
    "    plt.rcParams[\"figure.figsize\"] = [5, 5]\n",
    "    # plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(ind_raw[:,0], ind_raw[:,1], ind_raw[:,2], alpha = 0.5, c = 'r')\n",
    "    plt.show()\n",
    "\n",
    "def ele_list_gen(cr_c, mn_c, co_c, ni_c, num_c, mode = 'randchoice'):\n",
    "    np.random.seed()\n",
    "\n",
    "    assert abs(cr_c + mn_c + co_c + ni_c - 1) < 0.001, 'Make sure atomic ratio sum to 1'\n",
    "\n",
    "    while True:\n",
    "        if mode == 'randchoice':\n",
    "            len_cr = randrange(int(cr_c*num_c),int(cr_c*num_c)+2)\n",
    "            len_mn = randrange(int(mn_c*num_c),int(mn_c*num_c)+2)\n",
    "            len_co = randrange(int(co_c*num_c),int(co_c*num_c)+2)\n",
    "        elif mode == 'int':\n",
    "            len_cr = int(cr_c*num_c)\n",
    "            len_mn = int(mn_c*num_c)\n",
    "            len_co = int(co_c*num_c)\n",
    "        \n",
    "        len_ni = num_c-len_cr-len_mn-len_co\n",
    "        if (abs(len_ni-num_c*ni_c) <= 1\n",
    "            and abs(len_cr-num_c*cr_c) <= 1\n",
    "            and abs(len_mn-num_c*mn_c) <= 1\n",
    "            and abs(len_co-num_c*co_c) <= 1):\n",
    "            break\n",
    "\n",
    "    ele_list_raw = np.concatenate([np.zeros(len_cr)+2,np.ones(len_mn),0-np.ones(len_co),-1-np.ones(len_ni)],axis=0)\n",
    "    np.random.shuffle(ele_list_raw)\n",
    "    \n",
    "    return ele_list_raw\n",
    "\n",
    "def swap_step(action, state,):\n",
    "\n",
    "    a1 = action[0]\n",
    "    a2 = action[1]\n",
    "\n",
    "    state[a2], state[a1] = state[a1], state[a2]\n",
    "\n",
    "    return state\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(\"Created Directory : \", directory)\n",
    "    else:\n",
    "        print(\"Directory already existed : \", directory)\n",
    "    return directory\n",
    "\n",
    "class CE:\n",
    "    def __init__(self, \n",
    "        ind_1nn, ind_2nn, ind_3nn, ind_4nn, \n",
    "        ind_5nn, ind_6nn, ind_qua1nn, ind_qua1nn2nn,\n",
    "        ind_qua111122,\n",
    "\n",
    "        ind_trip111, ind_trip112, ind_trip113, ind_trip114,\n",
    "        ind_trip123, ind_trip125, ind_trip133, ind_trip134, \n",
    "        ind_trip135, ind_trip145, ind_trip155, ind_trip224,\n",
    "        ind_trip233, ind_trip255, ind_trip334, ind_trip335, \n",
    "        ind_trip345, ind_trip444, ind_trip455, \n",
    "\n",
    "        ind_1nn_pbc, ind_2nn_pbc, ind_3nn_pbc, ind_4nn_pbc, \n",
    "        ind_5nn_pbc, ind_6nn_pbc, ind_qua1nn_pbc, ind_qua1nn2nn_pbc, \n",
    "        ind_qua111122_pbc,\n",
    "\n",
    "        ind_trip111_pbc, ind_trip112_pbc, ind_trip113_pbc, ind_trip114_pbc,\n",
    "        ind_trip123_pbc, ind_trip125_pbc, ind_trip133_pbc, ind_trip134_pbc, \n",
    "        ind_trip135_pbc, ind_trip145_pbc, ind_trip155_pbc, ind_trip224_pbc,\n",
    "        ind_trip233_pbc, ind_trip255_pbc, ind_trip334_pbc, ind_trip335_pbc, \n",
    "        ind_trip345_pbc, ind_trip444_pbc, ind_trip455_pbc, \n",
    "        #* 21 quadruplets\n",
    "        qua_indlist_raw, qua_indlist_pbc, \n",
    "\n",
    "        ind_raw, use_pbc=True, merge_basis=False,\n",
    "        normalize_clusterfunc=False):\n",
    "\n",
    "        #* Inner of 32-atom config.\n",
    "        self.ind_1nn = ind_1nn\n",
    "        self.ind_2nn = ind_2nn\n",
    "        self.ind_3nn = ind_3nn\n",
    "        self.ind_4nn = ind_4nn\n",
    "        self.ind_5nn = ind_5nn\n",
    "        self.ind_6nn = ind_6nn\n",
    "        self.ind_qua1nn = ind_qua1nn\n",
    "        self.ind_qua1nn2nn = ind_qua1nn2nn\n",
    "        self.ind_qua111122 = ind_qua111122\n",
    "\n",
    "        #* Cluster on boundary of primary config.\n",
    "        self.ind_1nn_pbc = ind_1nn_pbc\n",
    "        self.ind_2nn_pbc = ind_2nn_pbc\n",
    "        self.ind_3nn_pbc = ind_3nn_pbc\n",
    "        self.ind_4nn_pbc = ind_4nn_pbc\n",
    "        self.ind_5nn_pbc = ind_5nn_pbc\n",
    "        self.ind_6nn_pbc = ind_6nn_pbc\n",
    "        self.ind_qua1nn_pbc = ind_qua1nn_pbc\n",
    "        self.ind_qua1nn2nn_pbc = ind_qua1nn2nn_pbc\n",
    "        self.ind_qua111122_pbc = ind_qua111122_pbc\n",
    "\n",
    "        #* Triplets.\n",
    "        self.ind_trip111, self.ind_trip111_pbc = ind_trip111, ind_trip111_pbc \n",
    "        self.ind_trip112, self.ind_trip112_pbc = ind_trip112, ind_trip112_pbc \n",
    "        self.ind_trip113, self.ind_trip113_pbc = ind_trip113, ind_trip113_pbc \n",
    "        self.ind_trip114, self.ind_trip114_pbc = ind_trip114, ind_trip114_pbc \n",
    "        self.ind_trip123, self.ind_trip123_pbc = ind_trip123, ind_trip123_pbc \n",
    "        self.ind_trip125, self.ind_trip125_pbc = ind_trip125, ind_trip125_pbc \n",
    "        self.ind_trip133, self.ind_trip133_pbc = ind_trip133, ind_trip133_pbc \n",
    "        self.ind_trip134, self.ind_trip134_pbc = ind_trip134, ind_trip134_pbc \n",
    "        self.ind_trip135, self.ind_trip135_pbc = ind_trip135, ind_trip135_pbc \n",
    "        self.ind_trip145, self.ind_trip145_pbc = ind_trip145, ind_trip145_pbc \n",
    "        self.ind_trip155, self.ind_trip155_pbc = ind_trip155, ind_trip155_pbc \n",
    "        self.ind_trip224, self.ind_trip224_pbc = ind_trip224, ind_trip224_pbc \n",
    "        self.ind_trip233, self.ind_trip233_pbc = ind_trip233, ind_trip233_pbc \n",
    "        self.ind_trip255, self.ind_trip255_pbc = ind_trip255, ind_trip255_pbc \n",
    "        self.ind_trip334, self.ind_trip334_pbc = ind_trip334, ind_trip334_pbc \n",
    "        self.ind_trip335, self.ind_trip335_pbc = ind_trip335, ind_trip335_pbc \n",
    "        self.ind_trip345, self.ind_trip345_pbc = ind_trip345, ind_trip345_pbc \n",
    "        self.ind_trip444, self.ind_trip444_pbc = ind_trip444, ind_trip444_pbc\n",
    "        self.ind_trip455, self.ind_trip455_pbc = ind_trip455, ind_trip455_pbc\n",
    "\n",
    "        #* Quadruplets\n",
    "        self.qua_indlist_raw, self.qua_indlist_pbc = qua_indlist_raw, qua_indlist_pbc\n",
    "\n",
    "        self.ind_raw = ind_raw\n",
    "        self.use_pbc = use_pbc\n",
    "        self.merge_basis = merge_basis\n",
    "        self.normalize_clusterfunc = normalize_clusterfunc\n",
    "\n",
    "        self.sym_oplist = np.array([2, 1, 1, 0, 6, 0, 4, 12, 24])\n",
    "        self.sym_optri = np.array([6, 3, 1])\n",
    "        self.sym_opqua = np.array([0, 0, 1, 0, 0, 0, 2, 0, 4])\n",
    "\n",
    "        '''Create list of all possible combination of embedded atoms\n",
    "        on clusters'''\n",
    "        pair_comb, tri_comb, qua_comb = [], [], []\n",
    "        for i, j, k, l in product([-2,-1,1,2], repeat=4):\n",
    "            pair_comb.append([i,j])\n",
    "            tri_comb.append([i,j,k])\n",
    "            qua_comb.append([i,j,k,l])\n",
    "\n",
    "        self.pair_comb = np.unique(pair_comb, axis=0)\n",
    "        self.tri_comb = np.unique(tri_comb, axis=0)\n",
    "        self.qua_comb = np.array(qua_comb)\n",
    "\n",
    "    #*Normalizaiton by the symmetry operation for each cluster\n",
    "    def sym_operator(self, cluster, mode='None'):\n",
    "        if mode == 'None':\n",
    "            mode_cluster = len(cluster)\n",
    "            uni, count_uni = np.unique(cluster, return_counts=True)\n",
    "            len_uni = len(uni)\n",
    "            count_uni = np.sort(count_uni)\n",
    "            mode_sym = mode_cluster - len(uni)\n",
    "            sym_op = self.sym_oplist[mode_sym+mode_cluster-2]\n",
    "\n",
    "        elif mode == 'tri1nn':\n",
    "            mode_cluster = len(cluster)\n",
    "            uni, count_uni = np.unique(cluster, return_counts=True)\n",
    "            len_uni = len(uni)\n",
    "            mode_sym = mode_cluster - len(uni)\n",
    "            sym_op = self.sym_optri[mode_sym]\n",
    "        \n",
    "        elif mode == 'qua1nn': #* 111111\n",
    "            uni, count_uni = np.unique(cluster, return_counts=True)\n",
    "            len_uni = len(uni)\n",
    "            count_uni = np.sort(count_uni)\n",
    "            len_count = len(count_uni)\n",
    "            c_ind = len_uni+len_count+count_uni[-1]-count_uni[0]\n",
    "            sym_op = self.sym_oplist[c_ind]\n",
    "\n",
    "        elif mode == 'qua1nn2nn': #* 111112\n",
    "            uni, count_uni = np.unique(cluster, return_counts=True)\n",
    "            len_uni = len(uni)\n",
    "            count_uni = np.sort(count_uni)\n",
    "            len_count = len(count_uni)\n",
    "            c_ind = len_uni+len_count+count_uni[-1]-count_uni[0]\n",
    "            single_list = np.array([2, 6, 8])\n",
    "            if c_ind in single_list:\n",
    "                sym_op = self.sym_opqua[c_ind]\n",
    "            else:\n",
    "                if c_ind == 4:\n",
    "                    if len(np.unique(cluster[:2])) == 2:\n",
    "                        sym_op = 4\n",
    "                    else:\n",
    "                        sym_op = 1\n",
    "\n",
    "                elif c_ind == 7:\n",
    "                    if len(np.unique(cluster[:2])) == 2 and len(np.unique(cluster[2:4])) == 2:\n",
    "                        sym_op = 4\n",
    "                    else:\n",
    "                        sym_op = 2\n",
    "\n",
    "            '''\n",
    "            Square configuration, could be verified by inputing cluster\n",
    "            [0,1,2,3] and check the number of symmetry operation.\n",
    "            '''\n",
    "        elif mode == 'qua111122': \n",
    "            sym_mat = np.tile(cluster,(8,1))\n",
    "            sym_mat[1] = np.concatenate([cluster[1:], cluster[:1]]) #* pi/2\n",
    "            sym_mat[2] = np.concatenate([cluster[2:], cluster[:2]]) #* pi\n",
    "            sym_mat[3] = np.concatenate([cluster[3:], cluster[:3]]) #* 3/2*pi\n",
    "            sym_mat[4][0], sym_mat[4][1], sym_mat[4][2], sym_mat[4][3] = (\n",
    "                cluster[1], cluster[0], cluster[3], cluster[2]\n",
    "            ) #* Reflection 1\n",
    "            sym_mat[5] = np.concatenate([sym_mat[4][2:], sym_mat[4][:2]]) #* Reflection 2\n",
    "            sym_mat[6] = np.concatenate([sym_mat[5][3:], sym_mat[5][:3]]) #* Centre 1\n",
    "            sym_mat[7] = np.concatenate([sym_mat[5][1:], sym_mat[5][:1]]) #* Centre 2\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "            ''' \n",
    "            From now on the coordinates-indices follow a \n",
    "            strictly defined ascending order.\n",
    "            Details can be checked in {qua_ind.py} file.\n",
    "            '''\n",
    "        elif (mode == '111133' or mode == '112233'\n",
    "            or mode == '112334' or mode == '113444'\n",
    "            or mode == '223334'):\n",
    "            #* Replace [1][2]\n",
    "            sym_mat = np.tile(cluster,(2,1))\n",
    "            sym_mat[1][1], sym_mat[1][2] = sym_mat[0][2], sym_mat[0][1]\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif mode == '111224' or mode == '133444':\n",
    "            #* Replace [2][3]\n",
    "            sym_mat = np.tile(cluster,(2,1))\n",
    "            sym_mat[1][2], sym_mat[1][3] = sym_mat[0][3], sym_mat[0][2]\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif mode == '111333' or mode == '222444':\n",
    "            sym_mat = np.tile(cluster,(6,1))\n",
    "            count = 0\n",
    "            for i, j, k in permutations(cluster[1:], 3):\n",
    "                sym_mat[count][1:] = np.array([i, j, k])\n",
    "                count += 1\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif mode == '111334':\n",
    "            sym_mat = np.tile(cluster,(2,1))\n",
    "            #* Replace [0][1]\n",
    "            sym_mat[1][0], sym_mat[1][1] = sym_mat[0][1], sym_mat[0][0]\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif mode == '113344':\n",
    "            sym_mat = np.tile(cluster,(4,1))\n",
    "            sym_mat[1] = cluster[1], cluster[0], cluster[3], cluster[2]\n",
    "            sym_mat[2] = cluster[2], cluster[3], cluster[0], cluster[1]\n",
    "            sym_mat[3] = cluster[3], cluster[2], cluster[1], cluster[0]\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif mode == '222244':\n",
    "            sym_mat = np.tile(cluster,(8,1))\n",
    "            sym_mat[1] = cluster[0], cluster[2], cluster[1], cluster[3]\n",
    "            sym_mat[2] = cluster[1], cluster[0], cluster[3], cluster[2]\n",
    "            sym_mat[3] = cluster[1], cluster[3], cluster[0], cluster[2]\n",
    "            sym_mat[4] = cluster[2], cluster[0], cluster[3], cluster[1]\n",
    "            sym_mat[5] = cluster[2], cluster[3], cluster[0], cluster[1]\n",
    "            sym_mat[6] = cluster[3], cluster[1], cluster[2], cluster[0]\n",
    "            sym_mat[7] = cluster[3], cluster[2], cluster[1], cluster[0]\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif mode == '223333':\n",
    "            sym_mat = np.tile(cluster,(8,1))\n",
    "            sym_mat[1] = cluster[0], cluster[3], cluster[2], cluster[1]\n",
    "            sym_mat[2] = cluster[1], cluster[0], cluster[3], cluster[2]\n",
    "            sym_mat[3] = cluster[1], cluster[2], cluster[3], cluster[0]\n",
    "            sym_mat[4] = cluster[2], cluster[1], cluster[0], cluster[3]\n",
    "            sym_mat[5] = cluster[2], cluster[3], cluster[0], cluster[1]\n",
    "            sym_mat[6] = cluster[3], cluster[0], cluster[1], cluster[2]\n",
    "            sym_mat[7] = cluster[3], cluster[2], cluster[1], cluster[0]\n",
    "\n",
    "            sym_op = len(np.unique(sym_mat, axis=0))\n",
    "\n",
    "        elif (mode == '111123' or mode == '111134'\n",
    "            or mode == '111233' or mode == '112234'\n",
    "            or mode == '112333' or mode == '113334'\n",
    "            or mode == '122334' or mode == '123333'):\n",
    "\n",
    "            sym_op = 1\n",
    "\n",
    "        return sym_op\n",
    "\n",
    "    def sym_op_basis(self, cluster_type, cpr):\n",
    "        \n",
    "        if self.merge_basis:\n",
    "            #* Merge the quadruplets' type.\n",
    "            if len(cluster_type) == 6:\n",
    "                if (cluster_type == '111133' or cluster_type == '112233'\n",
    "                    or cluster_type == '112334' or cluster_type == '113444'\n",
    "                    or cluster_type == '223334'):\n",
    "                    cluster_type = '111133'\n",
    "\n",
    "                elif cluster_type == '111224' or cluster_type == '133444':\n",
    "                    cluster_type = '111224'\n",
    "                \n",
    "                elif cluster_type == '111333' or cluster_type == '222444':\n",
    "                    cluster_type = '111333'\n",
    "                \n",
    "                elif (cluster_type == '111123' or cluster_type == '111134'\n",
    "                    or cluster_type == '111233' or cluster_type == '112234'\n",
    "                    or cluster_type == '112333' or cluster_type == '113334'\n",
    "                    or cluster_type == '122334' or cluster_type == '123333'):\n",
    "                    cluster_type = '111123'\n",
    "\n",
    "            if cluster_type == 'pair':\n",
    "                cpr_ = np.zeros(6)\n",
    "                cpr_[[0,1,2]] = cpr[[0,4,8]]\n",
    "                cpr_[3] = (cpr[1]+cpr[3])/2 #* phi12-phi21\n",
    "                cpr_[4] = (cpr[2]+cpr[6])/2 #* phi13-phi31\n",
    "                cpr_[5] = (cpr[5]+cpr[7])/2 #* phi23-phi32\n",
    "\n",
    "            elif cluster_type == '111':\n",
    "                cpr_ = np.zeros(10)\n",
    "                cpr_[[0,1,2]] = cpr[[0,13,26]] #* phi111\n",
    "                cpr_[3] = np.mean(cpr[[1,3,9]]) #* phi112\n",
    "                cpr_[4] = np.mean(cpr[[2,6,18]]) #* phi113\n",
    "                cpr_[5] = np.mean(cpr[[4,10,12]]) #* phi122\n",
    "                cpr_[6] = np.mean(cpr[[8,20,24]]) #* phi133\n",
    "                cpr_[7] = np.mean(cpr[[5,7,11,15,19,21]]) #* phi123\n",
    "                cpr_[8] = np.mean(cpr[[14,16,22]]) #* phi223\n",
    "                cpr_[9] = np.mean(cpr[[17,23,25]]) #* phi233\n",
    "\n",
    "            elif cluster_type == '112':\n",
    "                cpr_ = np.zeros(18)\n",
    "                cpr_[[0,1,2]] = cpr[[0,13,26]] #* phi111\n",
    "                cpr_[3] = np.mean(cpr[[1,3]]) #* phi112\n",
    "                cpr_[4] = np.mean(cpr[[2,6]]) #* phi113\n",
    "                cpr_[5] = np.mean(cpr[4]) #* phi122\n",
    "                cpr_[6] = cpr[8] #* phi133\n",
    "                cpr_[7] = np.mean(cpr[[5,7]]) #* phi123\n",
    "                cpr_[8] = np.mean(cpr[[14,16]]) #* phi223\n",
    "                cpr_[9] = cpr[17] #* phi233\n",
    "                cpr_[10] = cpr[9] #* phi211\n",
    "                cpr_[11] = np.mean(cpr[[10,12]]) #* phi212\n",
    "                cpr_[12] = np.mean(cpr[[11,15]]) #* phi213\n",
    "                cpr_[13] = cpr[18] #* phi311\n",
    "                cpr_[14] = np.mean(cpr[[19,21]]) #* phi312\n",
    "                cpr_[15] = np.mean(cpr[[20,24]]) #* phi313\n",
    "                cpr_[16] = cpr[22] #* phi322\n",
    "                cpr_[17] = np.mean(cpr[[23,25]]) #* phi323\n",
    "\n",
    "            elif cluster_type == '123':\n",
    "                cpr_ = cpr\n",
    "            \n",
    "            elif cluster_type == '111111':\n",
    "                \n",
    "                cpr_ = np.zeros(15)\n",
    "                cpr_[0] = cpr[0] #* phi1111\n",
    "                cpr_[1] = np.mean(cpr[[1,3,9,27]]) #* phi1112\n",
    "                cpr_[2] = np.mean(cpr[[2,6,18,54]]) #* phi1113\n",
    "                cpr_[3] = np.mean(cpr[[4,10,12,28,30,36]]) #* phi1122\n",
    "                cpr_[4] = np.mean(cpr[[5,7,11,15,19,21,29,33,45,55,57,63]]) #* phi1123\n",
    "                cpr_[5] = np.mean(cpr[[8,20,24,56,60,72]]) #* phi1133\n",
    "                cpr_[6] = np.mean(cpr[[13,31,37,39]]) #* phi1222\n",
    "                cpr_[7] = np.mean(cpr[[14,16,22,32,34,38,42,46,48,58,64,66]]) #* phi1223\n",
    "                cpr_[8] = np.mean(cpr[[17,23,25,35,47,51,59,61,65,69,73,75]]) #* phi1233\n",
    "                cpr_[9] = np.mean(cpr[[26,62,74,78]]) #* phi1333\n",
    "                cpr_[10] = cpr[40] #* phi2222\n",
    "                cpr_[11] = np.mean(cpr[[41,43,49,67]]) #* phi2223\n",
    "                cpr_[12] = np.mean(cpr[[44,50,52,68,70,76]]) #* phi2233\n",
    "                cpr_[13] = np.mean(cpr[[53,71,77,79]]) #* phi2333\n",
    "                cpr_[14] = cpr[80] #* phi3333\n",
    "\n",
    "            elif cluster_type == '111123':\n",
    "                cpr_ = cpr\n",
    "\n",
    "            else: #* Quaduplets other cases\n",
    "                pth = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/'\n",
    "                symeq_ = np.load(pth+f'ind_symeq_{cluster_type}.npy', allow_pickle=True)\n",
    "                cpr_ = np.zeros(len(symeq_))\n",
    "                for i in range(len(symeq_)):\n",
    "                    cpr_[i] = np.mean(cpr[np.array(symeq_[i])])\n",
    "\n",
    "            return cpr_\n",
    "\n",
    "        else:\n",
    "            return cpr\n",
    "\n",
    "    def phi1(self, x):\n",
    "        return 2/math.sqrt(10)*x\n",
    "\n",
    "    def phi2(self, x):\n",
    "        return -5/3 + 2/3*(x**2)\n",
    "\n",
    "    def phi3(self, x):\n",
    "        return -17/30*math.sqrt(10)*x + math.sqrt(10)/6*(x**3)\n",
    "    \n",
    "    def cpr(self, val_list, cluster_type):\n",
    "        ''' \n",
    "        Return the correlation function for each cluster.\n",
    "        Update 2301: For cluster function itself,\n",
    "        the symmetry operation must be applied.\n",
    "        '''\n",
    "        p1l = self.phi1(val_list).reshape(-1, 1)\n",
    "        p2l = self.phi2(val_list).reshape(-1, 1)\n",
    "        p3l = self.phi3(val_list).reshape(-1, 1)\n",
    "        pl = np.concatenate([p1l, p2l, p3l], 1).T\n",
    "        c_len = len(val_list)\n",
    "        atom = 1\n",
    "        for i in range(c_len):\n",
    "            atom_1 = pl[:, i]\n",
    "            atom = np.outer(atom_1, atom)\n",
    "\n",
    "        return self.sym_op_basis(cluster_type, atom.flatten())\n",
    "\n",
    "    def trip_extract(self, config, ind_trip_raw, ind_trip_pbc, type_trip, cpr):\n",
    "        #* Merge the pbc and raw indices list.\n",
    "        if self.use_pbc:\n",
    "            ind_trip_all = np.concatenate([ind_trip_raw, ind_trip_pbc], axis=0)\n",
    "        else:\n",
    "            ind_trip_all = ind_trip_raw\n",
    "\n",
    "        for i in ind_trip_all:\n",
    "            a1, a2, a3 = config[i[0]], config[i[1]], config[i[2]]\n",
    "            cluster = np.array([a1, a2, a3])\n",
    "\n",
    "            if type_trip == '111':\n",
    "                cpr += self.cpr(cluster, '111')/self.sym_operator(cluster, mode='tri1nn')\n",
    "\n",
    "            elif type_trip == '112':\n",
    "                #* Symmetry operation will be done only in AAB or ABC form\n",
    "                # print(a1, a2, a3)\n",
    "                if ((a1 == a2 and a1 != a3) \n",
    "                    or (a1 == a3 and a1 != a2) \n",
    "                    or (len(np.unique(cluster)) == 3)):\n",
    "                    cpr += self.cpr(cluster, '112')/2\n",
    "                else:\n",
    "                    cpr += self.cpr(cluster, '112')\n",
    "\n",
    "            elif type_trip == '123':\n",
    "                cpr += self.cpr(cluster, '123')\n",
    "        \n",
    "        if self.normalize_clusterfunc:\n",
    "            cpr = np.array(cpr)/len(ind_trip_all)\n",
    "        elif not self.normalize_clusterfunc:\n",
    "            cpr = np.array(cpr)\n",
    "\n",
    "        return cpr\n",
    "\n",
    "    def qua_extract(self, config, ind_qua_raw, ind_qua_pbc, type_qua, cpr=0):\n",
    "        #* Merge the pbc and raw indices list.\n",
    "        if self.use_pbc:\n",
    "            ind_qua_all = np.concatenate([ind_qua_raw, ind_qua_pbc], axis=0)\n",
    "        else:\n",
    "            ind_qua_all = ind_qua_raw\n",
    "\n",
    "        for i in ind_qua_all:\n",
    "            a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "            cluster = np.array([a1, a2, a3, a4])\n",
    "\n",
    "            cpr += self.cpr(cluster, type_qua)/self.sym_operator(cluster, mode=type_qua)\n",
    "\n",
    "        if self.normalize_clusterfunc:\n",
    "            cpr = np.array(cpr)/len(ind_qua_all)\n",
    "        elif not self.normalize_clusterfunc:\n",
    "            cpr = np.array(cpr)\n",
    "\n",
    "        return cpr\n",
    "\n",
    "    def cluster_extra(self, config, embed):\n",
    "        '''\n",
    "        Config in PBC must be in R = (N*27)x3 form.\n",
    "        '''\n",
    "\n",
    "        #! Very old version in pair correlation function, must\n",
    "        #! be updated in the future. (If have time I)\n",
    "        if embed['point']:\n",
    "            len_cr, len_mn, len_co, len_ni = (\n",
    "                len(config[config == 2]), len(config[config == 1]),\n",
    "                len(config[config == -1]), len(config[config == -2]),\n",
    "            )\n",
    "            cpr_point = np.sum(np.array([\n",
    "                [self.phi1(2), self.phi1(1), self.phi1(-1), self.phi1(-2)],\n",
    "                [self.phi2(2), self.phi2(1), self.phi2(-1), self.phi2(-2)],\n",
    "                [self.phi3(2), self.phi3(1), self.phi3(-1), self.phi3(-2)],\n",
    "            ])*np.array([len_cr, len_mn, len_co, len_ni]),\n",
    "            axis=1)\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_point = cpr_point/len(config)\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_point = cpr_point\n",
    "\n",
    "        cpr_1nn = 0\n",
    "        if embed['pair1']:\n",
    "            for i in self.ind_1nn:\n",
    "                a1, a2 = config[i[0]], config[i[1]]\n",
    "                cluster = np.array([a1, a2])\n",
    "                cpr_1nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_1nn_pbc:\n",
    "                    a1, a2 = config[i[0]], config[i[1]]\n",
    "                    cluster = np.array([a1, a2])\n",
    "                    cpr_1nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))  \n",
    "            \n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_1nn = np.array(cpr_1nn)/(len(self.ind_1nn)+len(self.ind_1nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_1nn = np.array(cpr_1nn)\n",
    "        else:\n",
    "            cpr_1nn = []\n",
    "\n",
    "        cpr_2nn = 0\n",
    "        if embed['pair2']:\n",
    "            for i in self.ind_2nn:\n",
    "                a1, a2 = config[i[0]], config[i[1]]\n",
    "                cluster = np.array([a1, a2])\n",
    "                cpr_2nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_2nn_pbc:\n",
    "                    a1, a2 = config[i[0]], config[i[1]]\n",
    "                    cluster = np.array([a1, a2])\n",
    "                    cpr_2nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_2nn = np.array(cpr_2nn)/(len(self.ind_2nn)+len(self.ind_2nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_2nn = np.array(cpr_2nn)\n",
    "\n",
    "        else:\n",
    "            cpr_2nn = []\n",
    "        \n",
    "        cpr_3nn = 0\n",
    "        if embed['pair3']:\n",
    "            for i in self.ind_3nn:\n",
    "                a1, a2 = config[i[0]], config[i[1]]\n",
    "                cluster = np.array([a1, a2])\n",
    "                cpr_3nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_3nn_pbc:\n",
    "                    a1, a2 = config[i[0]], config[i[1]]\n",
    "                    cluster = np.array([a1, a2])\n",
    "                    cpr_3nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_3nn = np.array(cpr_3nn)/(len(self.ind_3nn)+len(self.ind_3nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_3nn = np.array(cpr_3nn)\n",
    "        else:\n",
    "            cpr_3nn = []\n",
    "\n",
    "        cpr_4nn = 0\n",
    "        if embed['pair4']:\n",
    "            for i in self.ind_4nn:\n",
    "                a1, a2 = config[i[0]], config[i[1]]\n",
    "                cluster = np.array([a1, a2])\n",
    "                cpr_4nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_4nn_pbc:\n",
    "                    a1, a2 = config[i[0]], config[i[1]]\n",
    "                    cluster = np.array([a1, a2])\n",
    "                    cpr_4nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_4nn = np.array(cpr_4nn)/(len(self.ind_4nn)+len(self.ind_4nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_4nn = np.array(cpr_4nn)\n",
    "        else:\n",
    "            cpr_4nn = []\n",
    "\n",
    "        cpr_5nn = 0\n",
    "        if embed['pair5']:\n",
    "            for i in self.ind_5nn:\n",
    "                a1, a2 = config[i[0]], config[i[1]]\n",
    "                cluster = np.array([a1, a2])\n",
    "                cpr_5nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_5nn_pbc:\n",
    "                    a1, a2 = config[i[0]], config[i[1]]\n",
    "                    cluster = np.array([a1, a2])\n",
    "                    cpr_5nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_5nn = np.array(cpr_5nn)/(len(self.ind_5nn)+len(self.ind_5nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_5nn = np.array(cpr_5nn)\n",
    "        else:\n",
    "            cpr_5nn = []\n",
    "\n",
    "        cpr_6nn = 0\n",
    "        if embed['pair6']:\n",
    "            for i in self.ind_6nn:\n",
    "                a1, a2 = config[i[0]], config[i[1]]\n",
    "                cluster = np.array([a1, a2])\n",
    "                cpr_6nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_6nn_pbc:\n",
    "                    a1, a2 = config[i[0]], config[i[1]]\n",
    "                    cluster = np.array([a1, a2])\n",
    "                    cpr_6nn += (self.cpr(cluster, 'pair')/self.sym_operator(cluster))\n",
    "            \n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_6nn = np.array(cpr_6nn)/(len(self.ind_6nn)+len(self.ind_6nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_6nn = np.array(cpr_6nn)\n",
    "        else:\n",
    "            cpr_6nn = []\n",
    "            \n",
    "        cpr_tri111 = 0\n",
    "        if embed['tri111']:\n",
    "            cpr_tri111 = self.trip_extract(config, self.ind_trip111,\n",
    "                self.ind_trip111_pbc, '111', cpr_tri111)\n",
    "        else:\n",
    "            cpr_tri111 = []\n",
    "\n",
    "        cpr_tri444 = 0\n",
    "        if embed['tri444']:\n",
    "            cpr_tri444 = self.trip_extract(config, self.ind_trip444,\n",
    "                self.ind_trip444_pbc, '111', cpr_tri444)\n",
    "        else:\n",
    "            cpr_tri444 = []\n",
    "            # for i in self.ind_trip1nn:\n",
    "            #     a1, a2, a3 = config[i[0]], config[i[1]], config[i[2]]\n",
    "            #     cluster = np.array([a1, a2, a3])\n",
    "            #     cpr_tri1nn += (self.cpr(cluster)/self.sym_operator(cluster, mode='tri1nn')).tolist()\n",
    "\n",
    "            # for i in self.ind_trip1nn_pbc:\n",
    "            #     a1, a2, a3 = config[i[0]], config[i[1]], config[i[2]]\n",
    "            #     cluster = np.array([a1, a2, a3])\n",
    "            #     cpr_tri1nn += (self.cpr(cluster)/self.sym_operator(cluster, mode='tri1nn')).tolist()\n",
    "\n",
    "            # cpr_tri1nn = np.array(cpr_tri1nn)/(len(self.ind_trip1nn)+len(self.ind_trip1nn_pbc))\n",
    "\n",
    "        cpr_tri112 = 0\n",
    "        if embed['tri112']:\n",
    "            cpr_tri112 = self.trip_extract(config, self.ind_trip112,\n",
    "                self.ind_trip112_pbc, '112', cpr_tri112)\n",
    "        else:\n",
    "            cpr_tri112 = []\n",
    "        \n",
    "        cpr_tri113 = 0\n",
    "        if embed['tri113']:\n",
    "            cpr_tri113 = self.trip_extract(config, self.ind_trip113,\n",
    "                self.ind_trip113_pbc, '112', cpr_tri113)\n",
    "        else:\n",
    "            cpr_tri113 = []\n",
    "\n",
    "        cpr_tri114 = 0\n",
    "        if embed['tri114']:\n",
    "            cpr_tri114 = self.trip_extract(config, self.ind_trip114,\n",
    "                self.ind_trip114_pbc, '112', cpr_tri114)\n",
    "        else:\n",
    "            cpr_tri114 = []\n",
    "        \n",
    "        cpr_tri133 = 0\n",
    "        if embed['tri133']:\n",
    "            cpr_tri133 = self.trip_extract(config, self.ind_trip133,\n",
    "                self.ind_trip133_pbc, '112', cpr_tri133)\n",
    "        else:\n",
    "            cpr_tri133 = []\n",
    "\n",
    "        cpr_tri155 = 0\n",
    "        if embed['tri155']:\n",
    "            cpr_tri155 = self.trip_extract(config, self.ind_trip155,\n",
    "                self.ind_trip155_pbc, '112', cpr_tri155)\n",
    "        else:\n",
    "            cpr_tri155 = []\n",
    "\n",
    "        cpr_tri224 = 0\n",
    "        if embed['tri224']:\n",
    "            cpr_tri224 = self.trip_extract(config, self.ind_trip224,\n",
    "                self.ind_trip224_pbc, '112', cpr_tri224)\n",
    "        else:\n",
    "            cpr_tri224 = []\n",
    "        \n",
    "        cpr_tri233 = 0\n",
    "        if embed['tri233']:\n",
    "            cpr_tri233 = self.trip_extract(config, self.ind_trip233,\n",
    "                self.ind_trip233_pbc, '112', cpr_tri233)\n",
    "        else:\n",
    "            cpr_tri233 = []\n",
    "\n",
    "        cpr_tri255 = 0\n",
    "        if embed['tri255']:\n",
    "            cpr_tri255 = self.trip_extract(config, self.ind_trip255,\n",
    "                self.ind_trip255_pbc, '112', cpr_tri255)\n",
    "        else:\n",
    "            cpr_tri255 = []\n",
    "\n",
    "        cpr_tri334 = 0\n",
    "        if embed['tri334']:\n",
    "            cpr_tri334 = self.trip_extract(config, self.ind_trip334,\n",
    "                self.ind_trip334_pbc, '112', cpr_tri334)\n",
    "        else:\n",
    "            cpr_tri334 = []\n",
    "\n",
    "        cpr_tri335 = 0\n",
    "        if embed['tri335']:\n",
    "            cpr_tri335 = self.trip_extract(config, self.ind_trip335,\n",
    "                self.ind_trip335_pbc, '112', cpr_tri335)\n",
    "        else:\n",
    "            cpr_tri335 = []\n",
    "\n",
    "        cpr_tri455 = 0\n",
    "        if embed['tri455']:\n",
    "            cpr_tri455 = self.trip_extract(config, self.ind_trip455,\n",
    "                self.ind_trip455_pbc, '112', cpr_tri455)\n",
    "        else:\n",
    "            cpr_tri455 = []\n",
    "            # for i in self.ind_trip1nn2nn_pbc:\n",
    "            #     a1, a2, a3 = config[i[0]], config[i[1]], config[i[2]]\n",
    "            #     cluster = np.array([a1, a2, a3])\n",
    "            #     #* Symmetry operation will be doen only in AAB or ABC form\n",
    "            #     if ((a1 == a2 and a1 != a3) \n",
    "            #         or (a1 == a3 and a1 != a2) \n",
    "            #         or (np.unique(cluster) == 3)):\n",
    "            #         cpr_tri1nn2nn += (self.cpr(cluster)/2).tolist()\n",
    "            #     else:\n",
    "            #         cpr_tri1nn2nn += self.cpr(cluster).tolist()\n",
    "\n",
    "            # cpr_tri1nn2nn = np.array(cpr_tri1nn2nn)/(len(self.ind_trip1nn2nn)+len(self.ind_trip1nn2nn_pbc))\n",
    "\n",
    "        cpr_tri123 = 0\n",
    "        if embed['tri123']:\n",
    "            cpr_tri123 = self.trip_extract(config, self.ind_trip123,\n",
    "                self.ind_trip123_pbc, '123', cpr_tri123)\n",
    "        else:\n",
    "            cpr_tri123 = []\n",
    "\n",
    "        cpr_tri125 = 0\n",
    "        if embed['tri125']:\n",
    "            cpr_tri125 = self.trip_extract(config, self.ind_trip125,\n",
    "                self.ind_trip125_pbc, '123', cpr_tri125)\n",
    "        else:\n",
    "            cpr_tri125 = []\n",
    "\n",
    "        cpr_tri134 = 0\n",
    "        if embed['tri134']:\n",
    "            cpr_tri134 = self.trip_extract(config, self.ind_trip134,\n",
    "                self.ind_trip134_pbc, '123', cpr_tri134)\n",
    "        else:\n",
    "            cpr_tri134 = []\n",
    "\n",
    "        cpr_tri135 = 0\n",
    "        if embed['tri135']:\n",
    "            cpr_tri135 = self.trip_extract(config, self.ind_trip135,\n",
    "                self.ind_trip135_pbc, '123', cpr_tri135)\n",
    "        else:\n",
    "            cpr_tri135 = []\n",
    "\n",
    "        cpr_tri145 = 0\n",
    "        if embed['tri145']:\n",
    "            cpr_tri145 = self.trip_extract(config, self.ind_trip145,\n",
    "                self.ind_trip145_pbc, '123', cpr_tri145)\n",
    "        else:\n",
    "            cpr_tri145 = []\n",
    "\n",
    "        cpr_tri345 = 0\n",
    "        if embed['tri345']:\n",
    "            cpr_tri345 = self.trip_extract(config, self.ind_trip345,\n",
    "                self.ind_trip345_pbc, '123', cpr_tri345)\n",
    "        else:\n",
    "            cpr_tri345 = []\n",
    "\n",
    "        cpr_qua1nn = 0\n",
    "        if embed['qua111111']:\n",
    "            for i in self.ind_qua1nn:\n",
    "                a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "                cluster = np.array([a1, a2, a3, a4])\n",
    "                cpr_qua1nn += (self.cpr(cluster, '111111')/self.sym_operator(cluster, mode='qua1nn'))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_qua1nn_pbc:\n",
    "                    a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "                    cluster = np.array([a1, a2, a3, a4])\n",
    "                    cpr_qua1nn += (self.cpr(cluster, '111111')/self.sym_operator(cluster, mode='qua1nn'))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_qua1nn = np.array(cpr_qua1nn)/(len(self.ind_qua1nn) + len(self.ind_qua1nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_qua1nn = np.array(cpr_qua1nn)\n",
    "        else:\n",
    "            cpr_qua1nn = []\n",
    "\n",
    "        cpr_qua1nn2nn = 0\n",
    "        if embed['qua111112']:\n",
    "            for i in self.ind_qua1nn2nn:\n",
    "                a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "                cluster = np.array([a1, a2, a3, a4])\n",
    "                cpr_qua1nn2nn += (self.cpr(cluster, '111112')/self.sym_operator(cluster, mode='qua1nn2nn'))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_qua1nn2nn_pbc:\n",
    "                    a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "                    cluster = np.array([a1, a2, a3, a4])\n",
    "                    cpr_qua1nn2nn += (self.cpr(cluster, '111112')/self.sym_operator(cluster, mode='qua1nn2nn'))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_qua1nn2nn = np.array(cpr_qua1nn2nn)/(len(self.ind_qua1nn2nn)+len(self.ind_qua1nn2nn_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_qua1nn2nn = np.array(cpr_qua1nn2nn)\n",
    "        else:\n",
    "            cpr_qua1nn2nn = []\n",
    "\n",
    "        cpr_qua111122 = 0\n",
    "        if embed['qua111122']:\n",
    "            for i in self.ind_qua111122:\n",
    "                a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "                cluster = np.array([a1, a2, a3, a4])\n",
    "                cpr_qua111122 += (self.cpr(cluster, '111122')/self.sym_operator(cluster, mode='qua111122'))\n",
    "\n",
    "            if self.use_pbc:\n",
    "                for i in self.ind_qua111122_pbc:\n",
    "                    a1, a2, a3, a4 = config[i[0]], config[i[1]], config[i[2]], config[i[3]]\n",
    "                    cluster = np.array([a1, a2, a3, a4])\n",
    "                    cpr_qua111122 += (self.cpr(cluster, '111122')/self.sym_operator(cluster, mode='qua111122'))\n",
    "\n",
    "            if self.normalize_clusterfunc:\n",
    "                cpr_qua111122 = np.array(cpr_qua111122)/(len(self.ind_qua111122)+len(self.ind_qua111122_pbc))\n",
    "            elif not self.normalize_clusterfunc:\n",
    "                cpr_qua111122 = np.array(cpr_qua111122)\n",
    "        else:\n",
    "            cpr_qua111122 = []\n",
    "\n",
    "        '''\n",
    "        The cpr matrix for 21 quadruplet clusters\n",
    "        '''\n",
    "        cpr_quaremain = np.zeros((21, 81))\n",
    "        if self.merge_basis:\n",
    "            cpr_quaremain = np.load(\n",
    "                '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/qua_embed_new_zero.npy',\n",
    "                allow_pickle=True) #* Load the f****** empty g****** array\n",
    "\n",
    "        qua_remain_embed = [\n",
    "            '111123', '111133', '111134', '111224',\n",
    "            '111233', '111333', '111334', '112233',\n",
    "            '112234', '112333', '112334', '113334',\n",
    "            '113344', '113444', '122334', '123333',\n",
    "            '133444', '222244', '222444', '223333', '223334',\n",
    "        ]\n",
    "\n",
    "        effect_qua_ind = []\n",
    "        for i_ in range(len(qua_remain_embed)):\n",
    "            qua_type = qua_remain_embed[i_]\n",
    "            if embed[qua_type]:\n",
    "\n",
    "                q1,q2,q3,q4,q5,q6 = qua_type\n",
    "                cluster_type_raw = f'{q1}nn{q2}nn{q3}nn{q4}nn{q5}nn{q6}nn_raw'\n",
    "                cluster_type_pbc = f'{q1}nn{q2}nn{q3}nn{q4}nn{q5}nn{q6}nn_pbc'\n",
    "\n",
    "                cpr_quaremain[i_] = self.qua_extract(config, \n",
    "                    #* Dicts of indices for qua. clusters.\n",
    "                    self.qua_indlist_raw[cluster_type_raw], \n",
    "                    self.qua_indlist_pbc[cluster_type_pbc], \n",
    "                    qua_type)\n",
    "\n",
    "                effect_qua_ind.append(i_)\n",
    "\n",
    "        # np.save('./runs/demo/20230117_basis_cluster/debug.npy', cpr_quaremain)\n",
    "        # np.save('./runs/demo/20230117_basis_cluster/debug_ind.npy', effect_qua_ind)\n",
    "        #* R = 1 x (N x 81)\n",
    "        cpr_qua_remain = np.concatenate(cpr_quaremain[effect_qua_ind])\n",
    "\n",
    "        #* Return the concatenate array (R = 1 x sum(M))\n",
    "        return np.concatenate([\n",
    "            cpr_1nn, cpr_2nn, cpr_3nn, cpr_4nn,\n",
    "            cpr_tri111, cpr_tri112, cpr_tri113,\n",
    "            cpr_tri114, cpr_tri123, cpr_tri125,\n",
    "            cpr_tri133, cpr_tri134, cpr_tri135,\n",
    "            cpr_tri145, cpr_tri155, cpr_tri224, \n",
    "            cpr_tri233, cpr_tri255, cpr_tri334,\n",
    "            cpr_tri335, cpr_tri345, cpr_tri444, cpr_tri455, \n",
    "            cpr_qua1nn, cpr_qua1nn2nn, cpr_qua111122,\n",
    "            cpr_5nn, cpr_6nn, cpr_qua_remain, cpr_point,\n",
    "        ], 0)\n",
    "\n",
    "    def config_extra(self, num_cell, ind_cr, ind_mn, ind_co, ind_ni, embed_list):\n",
    "        ele_list = np.zeros(num_cell)\n",
    "        overlap_cr = find_overlap(self.ind_raw, ind_cr)\n",
    "        overlap_mn = find_overlap(self.ind_raw, ind_mn)\n",
    "        overlap_co = find_overlap(self.ind_raw, ind_co)\n",
    "        overlap_ni = find_overlap(self.ind_raw, ind_ni)\n",
    "\n",
    "        ele_list[np.where(overlap_cr)[0]] = 2\n",
    "        ele_list[np.where(overlap_mn)[0]] = 1\n",
    "        ele_list[np.where(overlap_co)[0]] = -1\n",
    "        ele_list[np.where(overlap_ni)[0]] = -2\n",
    "\n",
    "        #* Embedding form in PBC\n",
    "        ele_list_raw = ele_list.copy()\n",
    "        ele_list = np.tile(ele_list, 27)\n",
    "        cpr_list = self.cluster_extra(ele_list, embed_list)\n",
    "\n",
    "        return cpr_list, ele_list_raw\n",
    "\n",
    "    def read(self, incar_dir):\n",
    "        with open(incar_dir) as f:\n",
    "            input_strip = [s.strip() for s in f.readlines()]\n",
    "        return input_strip\n",
    "\n",
    "    '''\n",
    "    This function is for deducing the weight matrix\n",
    "    of the ideal condition (randomized state)\n",
    "    Cr, Mn, Co, Ni -> 2, 1, -1, -2\n",
    "    '''\n",
    "    def ideal_extract(self, atomic_ratio):\n",
    "        #* For pair cluster, their 'ideal weight' should be the same\n",
    "        #* in randomized states\n",
    "        atom_embed = np.array([2,1,-1,-2])\n",
    "        \n",
    "        #? Point\n",
    "        p_pointmat, point_cpr = [], []\n",
    "        point_phi1 = np.sum(self.phi1(atom_embed)*atomic_ratio)\n",
    "        point_phi2 = np.sum(self.phi2(atom_embed)*atomic_ratio)\n",
    "        point_phi3 = np.sum(self.phi3(atom_embed)*atomic_ratio)\n",
    "        weight_point = np.array([point_phi1, point_phi2, point_phi3])\n",
    "\n",
    "        if not self.normalize_clusterfunc:\n",
    "            weight_point = weight_point*len(self.ind_raw)\n",
    "\n",
    "        #? Pair\n",
    "        p_pairmat, pair_cpr = [], []\n",
    "        for pair_ in self.pair_comb:\n",
    "            a1, a2 = pair_ #* Embedded atom, e.g. 2,1\n",
    "            pair_cpr.append((self.cpr(pair_, 'pair')/self.sym_operator(pair_)).tolist())\n",
    "\n",
    "            ind_1, ind_2 = (\n",
    "                np.where(atom_embed==a1)[0][0], np.where(atom_embed==a2)[0][0])\n",
    "            c1, c2 = atomic_ratio[[ind_1, ind_2]]\n",
    "            p_pairmat.append([c1*c2])\n",
    "\n",
    "        #* 6 pair clusters.\n",
    "        weight_pair = np.tile(\n",
    "            np.sum(np.array(pair_cpr)*np.array(p_pairmat), axis=0), (6,1))\n",
    "\n",
    "        #* For triplet clusters, three general conditions could be\n",
    "        #* <111>*2 <112>*11 <123>*6\n",
    "        #* Order of type must follow with <embed_type>\n",
    "        trip_num_list = [['111'], ['112']*3, ['123']*2, \n",
    "                        ['112'], ['123']*3, ['112']*6,\n",
    "                        ['123'], ['111'], ['112']]\n",
    "        trip_num_list = [trip_item for sub_triplist in trip_num_list for trip_item in sub_triplist]\n",
    "        weight_trip = []\n",
    "\n",
    "        for sym_type in trip_num_list:\n",
    "            p_trimat, tri_cpr = [], []\n",
    "            #* For single type of triplet\n",
    "            for trip_ in self.tri_comb:\n",
    "\n",
    "                a1, a2, a3 = trip_\n",
    "                if sym_type == '111':\n",
    "                    tri_cpr.append(self.cpr(trip_, sym_type)/self.sym_operator(trip_, mode='tri1nn'))\n",
    "\n",
    "                elif sym_type == '112':\n",
    "                    #* Symmetry operation will be done only in AAB or ABC form\n",
    "                    # print(a1, a2, a3)\n",
    "                    if ((a1 == a2 and a1 != a3) \n",
    "                        or (a1 == a3 and a1 != a2) \n",
    "                        or (len(np.unique(trip_)) == 3)):\n",
    "                        tri_cpr.append(self.cpr(trip_, sym_type)/2)\n",
    "                    else:\n",
    "                        tri_cpr.append(self.cpr(trip_, sym_type))\n",
    "\n",
    "                elif sym_type == '123':\n",
    "                    tri_cpr.append(self.cpr(trip_, sym_type))\n",
    "\n",
    "                #* Content for each composed element.\n",
    "                ind_1, ind_2, ind_3 = (\n",
    "                    np.where(atom_embed==a1)[0][0], \n",
    "                    np.where(atom_embed==a2)[0][0],\n",
    "                    np.where(atom_embed==a3)[0][0],)\n",
    "                c1, c2, c3 = atomic_ratio[[ind_1, ind_2, ind_3]]\n",
    "\n",
    "                p_trimat.append([c1*c2*c3])\n",
    "            \n",
    "            weight_trip.append(np.sum(np.array(tri_cpr)*np.array(p_trimat), axis=0).tolist())\n",
    "\n",
    "        weight_trip = np.array(weight_trip, dtype=object)\n",
    "\n",
    "        #* For quadruplet clusters, basic operation is the \n",
    "        #* same as in triplets, while the symmetry operation \n",
    "        #* is way more complicated and must be defined one-by-one.\n",
    "        qua_num_list = ['qua111111', 'qua111112', 'qua111122',\n",
    "            #? Quadruplets x 21\n",
    "            '111123', '111133', '111134', '111224',\n",
    "            '111233', '111333', '111334', '112233',\n",
    "            '112234', '112333', '112334', '113334',\n",
    "            '113344', '113444', '122334', '123333',\n",
    "            '133444', '222244', '222444', '223333', '223334',]\n",
    "        weight_qua = []\n",
    "\n",
    "        for sym_type in qua_num_list:\n",
    "            p_quamat, qua_cpr = [], []\n",
    "            #* For single type of quadruplet\n",
    "            for qua_ in self.qua_comb:\n",
    "                a1, a2, a3, a4 = qua_\n",
    "                if sym_type == 'qua111111':\n",
    "                    qua_cpr.append(self.cpr(qua_, '111111')/self.sym_operator(qua_, mode='qua1nn'))\n",
    "                elif sym_type == 'qua111112':\n",
    "                    qua_cpr.append(self.cpr(qua_, '111112')/self.sym_operator(qua_, mode='qua1nn2nn'))    \n",
    "                elif sym_type == 'qua111122':\n",
    "                    qua_cpr.append(self.cpr(qua_, '111122')/self.sym_operator(qua_, mode='qua111122'))\n",
    "                else:\n",
    "                    qua_cpr.append(self.cpr(qua_, sym_type)/self.sym_operator(qua_, mode=sym_type))          \n",
    "\n",
    "                                #* Content for each composed element.\n",
    "                ind_1, ind_2, ind_3, ind_4 = (\n",
    "                    np.where(atom_embed==a1)[0][0], \n",
    "                    np.where(atom_embed==a2)[0][0],\n",
    "                    np.where(atom_embed==a3)[0][0],\n",
    "                    np.where(atom_embed==a4)[0][0],)\n",
    "                c1, c2, c3, c4 = atomic_ratio[[ind_1, ind_2, ind_3, ind_4]]\n",
    "\n",
    "                p_quamat.append([c1*c2*c3*c4])  \n",
    "\n",
    "            weight_qua.append(np.sum(np.array(qua_cpr)*np.array(p_quamat), axis=0).tolist())  \n",
    "\n",
    "        weight_qua = np.array(weight_qua, dtype=object)\n",
    "\n",
    "        len_list = []\n",
    "\n",
    "        #! DEBUG\n",
    "        # print(len_list) so far the length is correct.\n",
    "\n",
    "        weight_all = np.concatenate([\n",
    "            weight_pair[:4].flatten(),\n",
    "            np.concatenate(weight_trip),\n",
    "            np.concatenate(weight_qua[:3]),\n",
    "            weight_pair[4:].flatten(),\n",
    "            np.concatenate(weight_qua[3:]),\n",
    "            weight_point,\n",
    "        ], axis=0)\n",
    "\n",
    "        return weight_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the distribution of $MSAD^{1/2}$ value in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msad_list = []\n",
    "for i in range(1, 401):\n",
    "    cont_dir=f'/media/wz/7AD631A4D6316195/Projects/MSAD/outputs_32_crmnconi/Struc/CONTCAR{i}'\n",
    "    pos_dir=f'/media/wz/7AD631A4D6316195/Projects/MSAD/outputs_32_crmnconi/Struc/POSCAR{i}'\n",
    "\n",
    "    msad_list.append(np.sqrt(msad_ana_qua(cont_dir, pos_dir, 3.512, 2)['MSAD']))\n",
    "\n",
    "print(np.mean(msad_list))\n",
    "msad_list = np.array(msad_list)\n",
    "plt.hist(msad_list, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fitting part for MSAD\n",
    "\n",
    "Find the optimal set of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SA implementation for deducing best ECI combination.\n",
    "\n",
    "1. Operations: <code>Delete</code>, <code>Add</code>\n",
    "   \n",
    "   Operation on embedding list: 1 <-> 0\n",
    "\n",
    "2. Choose the best 10 embedding lists;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cluster indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(ind_rangeL, ind_rangeU, embed, atom_num):\n",
    "    energy_list, energy_list_ab = [], []\n",
    "    weight_list, msad_list = [], []\n",
    "    weight_list_ab, msad_list_ab = [], []\n",
    "    ele_list, ele_list_ab = [], []\n",
    "    for i in range(ind_rangeL, ind_rangeU):\n",
    "        ind_cr = np.load(f'/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/252525/Cr25Mn25Co25Ni2532_try{i}/Cr25Mn25Co25Ni25_cr_chosen.npy')\n",
    "        ind_mn = np.load(f'/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/252525/Cr25Mn25Co25Ni2532_try{i}/Cr25Mn25Co25Ni25_mn_chosen.npy')\n",
    "        ind_co = np.load(f'/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/252525/Cr25Mn25Co25Ni2532_try{i}/Cr25Mn25Co25Ni25_co_chosen.npy')\n",
    "        ind_ni = np.load(f'/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/252525/Cr25Mn25Co25Ni2532_try{i}/Cr25Mn25Co25Ni25_ni_chosen.npy')\n",
    "        \n",
    "        #* MSAD from DFT\n",
    "        cont_dir=f'/media/wz/7AD631A4D6316195/Projects/MSAD/outputs_32_crmnconi/configuration/CONTCAR{i}'\n",
    "        pos_dir=f'/media/wz/7AD631A4D6316195/Projects/MSAD/outputs_32_crmnconi/configuration/POSCAR{i}'\n",
    "\n",
    "        #* Free energy\n",
    "        osz_dir = f'/media/wz/7AD631A4D6316195/Projects/MSAD/outputs_32_crmnconi/Cr25Mn25Co25Ni25_3.512_try{i}/OSZICAR'\n",
    "        oszicar = ce_.read(osz_dir)[-1]\n",
    "        e_ground = float(''.join(oszicar).split()[-6]) \n",
    "        \n",
    "        #* Exclude the configurations whose variance between composition\n",
    "        #* is greater than 4.\n",
    "        if np.var([ \n",
    "            len(ind_cr), len(ind_mn), len(ind_co), len(ind_ni)\n",
    "            ]) <= 4:\n",
    "            #* Extract cluster info.\n",
    "            ce_feature, ele_ = ce_.config_extra(atom_num, ind_cr, ind_mn, ind_co, ind_ni, embed)\n",
    "\n",
    "            ele_list.append(ele_)\n",
    "            weight_list.append(ce_feature)\n",
    "            energy_list.append(e_ground)\n",
    "            msad_list.append(np.sqrt(msad_ana_qua(cont_dir, pos_dir, 3.512, 2)['MSAD']))\n",
    "        \n",
    "        elif np.var([ \n",
    "            len(ind_cr), len(ind_mn), len(ind_co), len(ind_ni)\n",
    "            ]) > 4 and len(ind_cr) >= 10:\n",
    "            #* The abnormal part, which occupies 10% of whole dataset.\n",
    "            ce_feature_ab, ele_ = ce_.config_extra(atom_num, ind_cr, ind_mn, ind_co, ind_ni, embed)\n",
    "\n",
    "            ele_list_ab.append(ele_)\n",
    "            weight_list_ab.append(ce_feature_ab)\n",
    "            energy_list_ab.append(e_ground)\n",
    "            msad_list_ab.append(np.sqrt(msad_ana_qua(cont_dir, pos_dir, 3.512, 2)['MSAD']))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Iter: {i}')\n",
    "        \n",
    "    msad_list, msad_list_ab = np.array(msad_list), np.array(msad_list_ab)\n",
    "    weight_list, weight_list_ab = np.array(weight_list), np.array(weight_list_ab) #* Per atom\n",
    "    return msad_list, weight_list, msad_list_ab, weight_list_ab, ele_list, ele_list_ab, energy_list, energy_list_ab\n",
    "\n",
    "#* Deducing step\n",
    "ind_1nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_1nn.npy')\n",
    "ind_2nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_2nn.npy')\n",
    "ind_3nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_3nn.npy')\n",
    "ind_4nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_4nn.npy')\n",
    "ind_5nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_5nn.npy')\n",
    "ind_6nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_6nn.npy')\n",
    "ind_qua1nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua1nn.npy')\n",
    "ind_qua1nn2nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua1nn2nn.npy')\n",
    "ind_qua111122 = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua111122_raw.npy')\n",
    "\n",
    "ind_1nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_1nn_pbc.npy')\n",
    "ind_2nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_2nn_pbc.npy')\n",
    "ind_3nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_3nn_pbc.npy')\n",
    "ind_4nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_4nn_pbc.npy')\n",
    "ind_5nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_5nn_pbc.npy')\n",
    "ind_6nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_6nn_pbc.npy')\n",
    "ind_qua1nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua1nn_pbc.npy')\n",
    "ind_qua1nn2nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua1nn2nn_pbc.npy')\n",
    "ind_qua111122_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua111122_pbc.npy')\n",
    "\n",
    "''' \n",
    "Thanks chatGPT for doing such *** things.\n",
    "'''\n",
    "pth_trip = '/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/triplet_ind/'\n",
    "\n",
    "ind_trip111 = np.load(f'{pth_trip}ind_trip111_raw.npy')\n",
    "ind_trip111_pbc = np.load(f'{pth_trip}ind_trip111_pbc.npy')\n",
    "ind_trip112 = np.load(f'{pth_trip}ind_trip112_raw.npy')\n",
    "ind_trip112_pbc = np.load(f'{pth_trip}ind_trip112_pbc.npy')\n",
    "ind_trip113 = np.load(f'{pth_trip}ind_trip113_raw.npy')\n",
    "ind_trip113_pbc = np.load(f'{pth_trip}ind_trip113_pbc.npy')\n",
    "ind_trip114 = np.load(f'{pth_trip}ind_trip114_raw.npy')\n",
    "ind_trip114_pbc = np.load(f'{pth_trip}ind_trip114_pbc.npy')\n",
    "ind_trip123 = np.load(f'{pth_trip}ind_trip123_raw.npy')\n",
    "ind_trip123_pbc = np.load(f'{pth_trip}ind_trip123_pbc.npy')\n",
    "ind_trip125 = np.load(f'{pth_trip}ind_trip125_raw.npy')\n",
    "ind_trip133 = np.load(f'{pth_trip}ind_trip133_raw.npy')\n",
    "ind_trip134 = np.load(f'{pth_trip}ind_trip134_raw.npy')\n",
    "ind_trip125_pbc = np.load(f'{pth_trip}ind_trip125_pbc.npy')\n",
    "ind_trip133_pbc = np.load(f'{pth_trip}ind_trip133_pbc.npy')\n",
    "ind_trip134_pbc = np.load(f'{pth_trip}ind_trip134_pbc.npy')\n",
    "ind_trip135 = np.load(f'{pth_trip}ind_trip135_raw.npy')\n",
    "ind_trip145 = np.load(f'{pth_trip}ind_trip145_raw.npy')\n",
    "ind_trip155 = np.load(f'{pth_trip}ind_trip155_raw.npy')\n",
    "ind_trip135_pbc = np.load(f'{pth_trip}ind_trip135_pbc.npy')\n",
    "ind_trip145_pbc = np.load(f'{pth_trip}ind_trip145_pbc.npy')\n",
    "ind_trip155_pbc = np.load(f'{pth_trip}ind_trip155_pbc.npy')\n",
    "ind_trip224 = np.load(f'{pth_trip}ind_trip224_raw.npy')\n",
    "ind_trip233 = np.load(f'{pth_trip}ind_trip233_raw.npy')\n",
    "ind_trip255 = np.load(f'{pth_trip}ind_trip255_raw.npy')\n",
    "ind_trip224_pbc = np.load(f'{pth_trip}ind_trip224_pbc.npy')\n",
    "ind_trip233_pbc = np.load(f'{pth_trip}ind_trip233_pbc.npy')\n",
    "ind_trip255_pbc = np.load(f'{pth_trip}ind_trip255_pbc.npy')\n",
    "ind_trip334 = np.load(f'{pth_trip}ind_trip334_raw.npy')\n",
    "ind_trip335 = np.load(f'{pth_trip}ind_trip335_raw.npy')\n",
    "ind_trip345 = np.load(f'{pth_trip}ind_trip345_raw.npy')\n",
    "ind_trip334_pbc = np.load(f'{pth_trip}ind_trip334_pbc.npy')\n",
    "ind_trip335_pbc = np.load(f'{pth_trip}ind_trip335_pbc.npy')\n",
    "ind_trip345_pbc = np.load(f'{pth_trip}ind_trip345_pbc.npy')\n",
    "ind_trip444 = np.load(f'{pth_trip}ind_trip444_raw.npy')\n",
    "ind_trip455 = np.load(f'{pth_trip}ind_trip455_raw.npy')\n",
    "ind_trip444_pbc = np.load(f'{pth_trip}ind_trip444_pbc.npy')\n",
    "ind_trip455_pbc = np.load(f'{pth_trip}ind_trip455_pbc.npy')\n",
    "\n",
    "#* 21 x quadruplets\n",
    "pth_sav = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221216_msadGA/'\n",
    "\n",
    "with open(f'{pth_sav}ind_quapbc.json', 'r') as f:\n",
    "    ind_quapbc = json.load(f)\n",
    "\n",
    "with open(f'{pth_sav}ind_quaraw.json', 'r') as f:\n",
    "    ind_quaraw = json.load(f)\n",
    "\n",
    "ind_raw = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_raw32.npy')\n",
    "\n",
    "ce_ = CE(ind_1nn, ind_2nn, ind_3nn, ind_4nn, ind_5nn, ind_6nn,\n",
    "        ind_qua1nn, ind_qua1nn2nn, ind_qua111122,\n",
    "\n",
    "        ind_trip111, ind_trip112, ind_trip113, ind_trip114,\n",
    "        ind_trip123, ind_trip125, ind_trip133, ind_trip134, \n",
    "        ind_trip135, ind_trip145, ind_trip155, ind_trip224,\n",
    "        ind_trip233, ind_trip255, ind_trip334, ind_trip335, \n",
    "        ind_trip345, ind_trip444, ind_trip455, \n",
    "\n",
    "        ind_1nn_pbc, ind_2nn_pbc, ind_3nn_pbc, ind_4nn_pbc, \n",
    "        ind_5nn_pbc, ind_6nn_pbc,\n",
    "        ind_qua1nn_pbc, ind_qua1nn2nn_pbc, ind_qua111122_pbc,\n",
    "\n",
    "        ind_trip111_pbc, ind_trip112_pbc, ind_trip113_pbc, ind_trip114_pbc,\n",
    "        ind_trip123_pbc, ind_trip125_pbc, ind_trip133_pbc, ind_trip134_pbc, \n",
    "        ind_trip135_pbc, ind_trip145_pbc, ind_trip155_pbc, ind_trip224_pbc,\n",
    "        ind_trip233_pbc, ind_trip255_pbc, ind_trip334_pbc, ind_trip335_pbc, \n",
    "        ind_trip345_pbc, ind_trip444_pbc, ind_trip455_pbc, \n",
    "\n",
    "        ind_quaraw, ind_quapbc, ind_raw, use_pbc=True, merge_basis=True,\n",
    "        normalize_clusterfunc=False)\n",
    "\n",
    "embed_val = np.ones(50).astype(bool)\n",
    "\n",
    "embed_type = [\n",
    "    'pair1', 'pair2', 'pair3', 'pair4', \n",
    "    'tri111', 'tri112', 'tri113', 'tri114',\n",
    "    'tri123', 'tri125', 'tri133', 'tri134',\n",
    "    'tri135', 'tri145', 'tri155', 'tri224',\n",
    "    'tri233', 'tri255', 'tri334', 'tri335',\n",
    "    'tri345', 'tri444', 'tri455',\n",
    "    'qua111111', 'qua111112', 'qua111122',\n",
    "    'pair5', 'pair6',\n",
    "    #? Quadruplets x 21\n",
    "    '111123', '111133', '111134', '111224',\n",
    "    '111233', '111333', '111334', '112233',\n",
    "    '112234', '112333', '112334', '113334',\n",
    "    '113344', '113444', '122334', '123333',\n",
    "    '133444', '222244', '222444', '223333', '223334', 'point',]\n",
    "\n",
    "#* Lookup table of the indices in weight for each cluster.\n",
    "\n",
    "# #* Without considering the cluster function's symmetry.\n",
    "# embed_book = [\n",
    "#     [i for i in range(0,9)],\n",
    "#     [i for i in range(9,18)],\n",
    "#     [i for i in range(18,27)],\n",
    "#     [i for i in range(27,36)],\n",
    "#     [i for i in range(36,63)],\n",
    "#     [i for i in range(63,90)],\n",
    "#     [i for i in range(90,117)],\n",
    "#     [i for i in range(117,144)],\n",
    "#     [i for i in range(144,171)],\n",
    "#     [i for i in range(171,198)],\n",
    "#     [i for i in range(198,225)],\n",
    "#     [i for i in range(225,252)],\n",
    "#     [i for i in range(252,279)],\n",
    "#     [i for i in range(279,306)],\n",
    "#     [i for i in range(306,333)],\n",
    "#     [i for i in range(333,360)],\n",
    "#     [i for i in range(360,387)],\n",
    "#     [i for i in range(387,414)],\n",
    "#     [i for i in range(414,441)],\n",
    "#     [i for i in range(441,468)],\n",
    "#     [i for i in range(468,495)],\n",
    "#     [i for i in range(495,522)],\n",
    "#     [i for i in range(522,549)],\n",
    "#     [i for i in range(549,630)],\n",
    "#     [i for i in range(630,711)],\n",
    "#     [i for i in range(711, 792)],\n",
    "#     [i for i in range(792, 801)],\n",
    "#     [i for i in range(801, 810)],\n",
    "# ]\n",
    "\n",
    "# qua_embed = np.linspace(810, 810+81*21-1, 81*21).reshape(21, 81).astype(int).tolist()\n",
    "# embed_book += qua_embed\n",
    "\n",
    "#* Considering the cluster function's symmetry\n",
    "embed_book = np.load(\n",
    "    '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/embed_book_new_230212.npy',\n",
    "    allow_pickle = True)\n",
    "\n",
    "embed_list = dict(zip(embed_type, embed_val))\n",
    "embed_book = np.array(embed_book, dtype=object)\n",
    "\n",
    "\n",
    "#* Re-calculate the weight_list and MSAD_list,\n",
    "#* Time comsuming better not to do that.\n",
    "# msad_list, weight_list, msad_list_ab, weight_list_ab, ele_list, ele_list_ab, energy_list, energy_list_ab = extract_weight(1, 401, embed_list, atom_num=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(energy_list, msad_list)\n",
    "\n",
    "plt.title('$E_0$ - $MSAD^{1/2}$')\n",
    "plt.xlabel('$E_0 (eV)$')\n",
    "plt.ylabel('$MSAD^{1/2} (pm)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save correlation functions and MSAD value list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_sav = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/ver2_energy_MSAD/runs/20230212/'\n",
    "np.save(f'{pth_sav}weight_50_sym', weight_list)\n",
    "np.save(f'{pth_sav}msad_list', msad_list)\n",
    "np.save(f'{pth_sav}weight_50_raw_abnormal_sym', weight_list_ab)\n",
    "np.save(f'{pth_sav}msad_list_abnormal', msad_list_ab)\n",
    "np.save(f'{pth_sav}ele_list_all', np.concatenate([ele_list, ele_list_ab], axis=0))\n",
    "np.save(f'{pth_sav}energy_list', energy_list)\n",
    "np.save(f'{pth_sav}energy_list_abnormal', energy_list_ab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main\n",
    "\n",
    "Block 1: GA implementation. (recommend)\n",
    "\n",
    "Block 2: SA implementation. (much simpler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config-MSAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class genetic:\n",
    "    def __init__(self, mut_prob, group_size, embed_book, \n",
    "            weight, weight_ab, msad, msad_ab, use_scalar=False):\n",
    "\n",
    "        self.mut_prob = mut_prob\n",
    "        self.group_size = group_size\n",
    "        self.embed_book = embed_book\n",
    "        #* Weight parameters in full length\n",
    "        self.weight = weight\n",
    "        self.weight_ab = weight_ab\n",
    "        self.msad_list = cp.asarray(msad)\n",
    "        self.msad_list_ab = cp.asarray(msad_ab)\n",
    "        self.use_scalar = use_scalar\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    #* For a single embedding list.\n",
    "    @staticmethod\n",
    "    def embed_extra(embed_book, weight_raw, embed_list):\n",
    "        #* Extract the weight\n",
    "        false_ind = np.where(embed_list==0)[0]\n",
    "        if len(false_ind) >= 1:\n",
    "            delete_ind = np.concatenate(embed_book[false_ind], axis=0)\n",
    "            weight_n = np.delete(weight_raw, delete_ind, axis=1)\n",
    "            #* Mean and Variance\n",
    "        else:\n",
    "            weight_n = weight_raw\n",
    "            \n",
    "        return weight_n\n",
    "\n",
    "    #* Apply scalar for each sub-cluster.\n",
    "    @staticmethod\n",
    "    def scalar_(weight_list):\n",
    "        scaler = StandardScaler().fit(weight_list)\n",
    "        return scaler\n",
    "\n",
    "    def fitness(self, embed_list):\n",
    "        fit_list = []\n",
    "        ''' embed_list (N x M) -> real weights -> score'''\n",
    "        for i in range(len(embed_list)):\n",
    "            #* Extract the weight\n",
    "            false_ind = np.where(embed_list[i]==0)[0]\n",
    "            if len(false_ind) >= 1:\n",
    "                delete_ind = np.concatenate(self.embed_book[false_ind])\n",
    "                weight_n = np.delete(self.weight, delete_ind, axis=1)\n",
    "                weight_n_ab = np.delete(self.weight_ab, delete_ind, axis=1)\n",
    "            else:\n",
    "                weight_n = self.weight\n",
    "                weight_n_ab = self.weight_ab\n",
    "            #* Return the score (fitness)\n",
    "            #? NEW version: train on splited set and do cv on test\n",
    "            #? with repeating 5 times. \n",
    "            n_samples = weight_n.shape[0]\n",
    "            # regr = LassoCV(alphas=np.logspace(-2,3,5), cv=5,\n",
    "            #     max_iter=2000)\n",
    "            regr = Lasso(alpha=0.05, max_iter=2000)\n",
    "            n_fold = 10 #* n-fold CV\n",
    "\n",
    "            #* Use dynamic scalar\n",
    "            if self.use_scalar:\n",
    "                scalar_n = genetic.scalar_(\n",
    "                    np.concatenate([weight_n, weight_n_ab], axis=0))\n",
    "                weight_n = scalar_n.transform(weight_n)\n",
    "                weight_n_ab = scalar_n.transform(weight_n_ab)\n",
    "\n",
    "            cv = ShuffleSplit(n_splits=n_fold, test_size=0.05, random_state=0)\n",
    "\n",
    "            #* GPU\n",
    "            weight_n_ab = cp.asarray(weight_n_ab)\n",
    "            msad_list_ab = cp.asarray(self.msad_list_ab)\n",
    "            score_list = cp.empty(0)\n",
    "\n",
    "            for train, test in cv.split(np.arange(n_samples)):\n",
    "                #* Move to GPU\n",
    "                weight_train, msad_train = cp.asarray(weight_n[train]), self.msad_list[train]\n",
    "                weight_test, msad_test = cp.asarray(weight_n[test]), self.msad_list[test]\n",
    "                # print(weight_test.shape)\n",
    "                #* Insert the abnormal dist. \n",
    "                #* Result in: 10% test set + abnormal set.\n",
    "                weight_test = cp.concatenate([weight_test, weight_n_ab], axis=0)\n",
    "                msad_test = cp.concatenate([msad_test, msad_list_ab], axis=0)\n",
    "\n",
    "                regr.fit(weight_train, msad_train)\n",
    "                score_list = cp.append(score_list, regr.score(weight_test, msad_test))\n",
    "\n",
    "            score = np.mean(cp.asnumpy(score_list))\n",
    "            fit_list.append(score)\n",
    "\n",
    "        fit_list = np.array(fit_list)\n",
    "        fit_list_raw = fit_list.copy()\n",
    "        #* Apply a softmax before selecting\n",
    "        fit_list = genetic.softmax(fit_list)\n",
    "        #* Reinforce the prob.?\n",
    "        fit_list = np.power(np.array(fit_list), 1)\n",
    "        #* Probability of survive\n",
    "        ind_h2l = np.argsort(fit_list)[::-1]\n",
    "        #* Weight with the highest score.\n",
    "        p0 = (fit_list/np.sum(fit_list))[ind_h2l[0]]\n",
    "        best_embed = embed_list[ind_h2l[0]]\n",
    "        best_score = fit_list_raw[ind_h2l[0]]\n",
    "\n",
    "        p_list = np.zeros(len(ind_h2l))\n",
    "        for i in range(len(ind_h2l)):\n",
    "            p_list[ind_h2l[i]] = p0*(1-p0)**i\n",
    "        \n",
    "        if len(fit_list) > self.group_size:\n",
    "            ind_survive = np.random.choice(np.arange(len(fit_list)), self.group_size, p=p_list/np.sum(p_list), replace=False)\n",
    "            embed_list = embed_list[ind_survive]\n",
    "            \n",
    "        #* Return the score \n",
    "        return embed_list, np.max(fit_list_raw), np.min(fit_list_raw), best_embed, best_score\n",
    "\n",
    "    @staticmethod\n",
    "    def multi(embed_list, breeding_list, count, re_time):\n",
    "\n",
    "        #* BREEDING(\n",
    "        if count >= len(breeding_list):\n",
    "            breeding_list_chosen = breeding_list[np.random.choice(\n",
    "                np.arange(len(breeding_list)), size=2, replace=False)\n",
    "            ]\n",
    "            embed_list = np.concatenate([ \n",
    "                embed_list, breeding_list_chosen\n",
    "            ], axis=0)\n",
    "\n",
    "        ind_list = np.arange(len(embed_list))\n",
    "        embed_list_n = []\n",
    "\n",
    "        for _ in range(re_time):\n",
    "            \n",
    "            chosen_lists = embed_list[np.random.choice(ind_list, size=2, replace=True)]\n",
    "            chosen_lists[0][-1] = 1\n",
    "            chosen_lists[1][-1] = 1\n",
    "            #* Length of multiplication part\n",
    "            len_multi = np.random.randint(\n",
    "                len(embed_list//2)-len(embed_list//8), len(embed_list//2)+len(embed_list//8)+1\n",
    "            )\n",
    "            # print(chosen_lists)\n",
    "            if _%2 == 0:\n",
    "                chosen_lists[0][len_multi:], chosen_lists[1][len_multi:] = (\n",
    "                    chosen_lists[1][len_multi:], chosen_lists[0][len_multi:])\n",
    "            elif _%2 == 1:\n",
    "                chosen_lists[1][:len_multi], chosen_lists[0][:len_multi] = (\n",
    "                    chosen_lists[0][:len_multi], chosen_lists[1][:len_multi])\n",
    "            embed_list_n += chosen_lists.tolist()\n",
    "\n",
    "        # #* Avoid the 0-list\n",
    "        # nan_ind = np.where(np.linalg.norm(embed_list, axis=1)==0)[0]\n",
    "        # if len(nan_ind) >= 1:\n",
    "        #     for nan in nan_ind:\n",
    "        #         embed_list[nan][0] = 1\n",
    "\n",
    "        #* Return R = re_time*2 X len matrix\n",
    "        return np.unique(np.array(embed_list_n), axis=0)\n",
    "\n",
    "    def mut(self, embed_list):\n",
    "        #* Better to set j at relatively low level\n",
    "        for i in range(len(embed_list)):\n",
    "            for j in range(1): #* Repeat twice for mutation.\n",
    "                if np.random.rand() < self.mut_prob:\n",
    "                    rand_ind = np.random.randint(len(embed_list[0]))\n",
    "                    val = embed_list[i][rand_ind]\n",
    "                    embed_list[i][rand_ind] = np.abs(val-1)\n",
    "                    embed_list[i][-1] = 1\n",
    "\n",
    "        # #* Avoid the 0-list\n",
    "        # nan_ind = np.where(np.linalg.norm(embed_list, axis=1)==0)[0]\n",
    "        # if len(nan_ind) >= 1:\n",
    "        #     for nan in nan_ind:\n",
    "        #         embed_list[nan][0] = 1\n",
    "\n",
    "        return np.unique(np.array(embed_list), axis=0)\n",
    "\n",
    "pth_load = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/ver2_energy_MSAD/runs/20230212/'\n",
    "\n",
    "#* Load correlation functions\n",
    "weight_pth = pth_load+'weight_50_sym.npy'\n",
    "weight_list = np.load(weight_pth)\n",
    "weight_pth_ab = pth_load+'weight_50_raw_abnormal_sym.npy'\n",
    "weight_list_ab = np.load(weight_pth_ab)\n",
    "#* Standarize features\n",
    "weight_list_all = np.concatenate([weight_list, weight_list_ab], axis=0)\n",
    "\n",
    "#* Load MSAD lists\n",
    "msad_pth = pth_load+'msad_list.npy'\n",
    "msad_list = np.load(msad_pth)\n",
    "msad_pth_ab = pth_load+'msad_list_abnormal.npy'\n",
    "msad_list_ab = np.load(msad_pth_ab)\n",
    "\n",
    "#* Load embed book\n",
    "embed_pth = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/embed_book_new_230212.npy'\n",
    "embed_book = np.load(embed_pth, allow_pickle=True)\n",
    "\n",
    "#* Normalization of weight list.\n",
    "dyna_scalar = False\n",
    "use_scalar = True\n",
    "if (not dyna_scalar) and use_scalar:\n",
    "    scaler = StandardScaler().fit(weight_list_all)\n",
    "    weight_list = scaler.transform(weight_list)\n",
    "    weight_list_ab = scaler.transform(weight_list_ab)\n",
    "\n",
    "#* Hyperparams for GA.\n",
    "trial_step = 2000\n",
    "mut_prob, group_size = 0.06, 30\n",
    "len_cluster = 50\n",
    "re_time_multi = 45\n",
    "len_chosenindividual = 10\n",
    "best_embed, best_score = np.zeros((len_chosenindividual, len_cluster)), -np.ones(len_chosenindividual)*2\n",
    "#* Initialize the genetic algo.\n",
    "gen = genetic(mut_prob, group_size, embed_book, \n",
    "    weight_list, weight_list_ab, msad_list, msad_list_ab,\n",
    "    use_scalar = dyna_scalar)\n",
    "\n",
    "#* Initialize the embedding list, R = 30xlen(cluster).\n",
    "embed_val = np.tile(np.concatenate([np.ones(8), np.zeros(len_cluster-8)]), group_size)\n",
    "np.random.shuffle(embed_val)\n",
    "embed_val = embed_val.reshape(group_size, len_cluster)\n",
    "# embed_val = best_embed_ #* Continue computing.\n",
    "\n",
    "score_h, score_l = [], []\n",
    "count = 0\n",
    "\n",
    "for i in range(trial_step):\n",
    "    \n",
    "    embed_val = np.unique(embed_val, axis=0) #* Remove duplicates.\n",
    "    embed_val = gen.multi(embed_val, best_embed, count, re_time=re_time_multi) #* Multiplication\n",
    "    embed_val = gen.mut(embed_val) #* Mutation\n",
    "    embed_val, max_score, min_score, top_embed, top_score = gen.fitness(embed_val) #* Natural Selection\n",
    "\n",
    "    #* Updating the embed list with best performance.\n",
    "    if top_score > np.max(best_score):\n",
    "        best_embed[count%len_chosenindividual] = top_embed\n",
    "        best_score[count%len_chosenindividual] = top_score\n",
    "        count += 1\n",
    "\n",
    "    clear_output(True)\n",
    "    score_h.append(max_score)\n",
    "    score_l.append(min_score)\n",
    "    if i%5 == 0:\n",
    "        x_axis = np.linspace(0, len(score_h)-1, len(score_h))\n",
    "        plt.plot(x_axis, score_h)\n",
    "        plt.plot(x_axis, score_l)\n",
    "        plt.title(f'CE-MSAD Iter: {i}, Max Score: {np.max(score_h)}')\n",
    "        plt.fill_between(x_axis, score_l, score_h, alpha=0.2, color=cm.viridis(0.5))\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config-Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class genetic:\n",
    "    def __init__(self, mut_prob, group_size, embed_book, \n",
    "            weight, weight_ab, energy, energy_ab, use_scalar=False):\n",
    "\n",
    "        self.mut_prob = mut_prob\n",
    "        self.group_size = group_size\n",
    "        self.embed_book = embed_book\n",
    "        #* Weight parameters in full length\n",
    "        self.weight = weight\n",
    "        self.weight_ab = weight_ab\n",
    "        self.energy_list = cp.asarray(energy)\n",
    "        self.energy_list_ab = cp.asarray(energy_ab)\n",
    "        self.use_scalar = use_scalar\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    #* For a single embedding list.\n",
    "    @staticmethod\n",
    "    def embed_extra(embed_book, weight_raw, embed_list):\n",
    "        #* Extract the weight\n",
    "        false_ind = np.where(embed_list==0)[0]\n",
    "        if len(false_ind) >= 1:\n",
    "            delete_ind = np.concatenate(embed_book[false_ind], axis=0)\n",
    "            weight_n = np.delete(weight_raw, delete_ind, axis=1)\n",
    "            #* Mean and Variance\n",
    "        else:\n",
    "            weight_n = weight_raw\n",
    "            \n",
    "        return weight_n\n",
    "\n",
    "    #* Apply scalar for each sub-cluster.\n",
    "    @staticmethod\n",
    "    def scalar_(weight_list):\n",
    "        scaler = StandardScaler().fit(weight_list)\n",
    "        return scaler\n",
    "\n",
    "    def fitness(self, embed_list, fitness_factor='R2'):\n",
    "        fit_list = []\n",
    "        ''' embed_list (N x M) -> real weights -> score'''\n",
    "        for i in range(len(embed_list)):\n",
    "            #* Extract the weight\n",
    "            false_ind = np.where(embed_list[i]==0)[0]\n",
    "            if len(false_ind) >= 1:\n",
    "                delete_ind = np.concatenate(self.embed_book[false_ind])\n",
    "                weight_n = np.delete(self.weight, delete_ind, axis=1)\n",
    "                weight_n_ab = np.delete(self.weight_ab, delete_ind, axis=1)\n",
    "            else:\n",
    "                weight_n = self.weight\n",
    "                weight_n_ab = self.weight_ab\n",
    "            #* Return the score (fitness)\n",
    "            #? NEW version: train on splited set and do cv on test\n",
    "            #? with repeating 5 times. \n",
    "            n_samples = weight_n.shape[0]\n",
    "            # regr = LassoCV(alphas=np.logspace(-2,3,5), cv=5,\n",
    "            #     max_iter=2000)\n",
    "            # regr = LinearRegression(fit_intercept=False)\n",
    "            regr = Lasso(alpha=0.05, max_iter=4000)\n",
    "            n_fold = 10 #* n-fold CV\n",
    "\n",
    "            #* Use dynamic scalar\n",
    "            if self.use_scalar:\n",
    "                scalar_n = genetic.scalar_(\n",
    "                    np.concatenate([weight_n, weight_n_ab], axis=0))\n",
    "                weight_n = scalar_n.transform(weight_n)\n",
    "                weight_n_ab = scalar_n.transform(weight_n_ab)\n",
    "\n",
    "            cv = ShuffleSplit(n_splits=n_fold, test_size=0.05, random_state=0)\n",
    "\n",
    "            #* GPU\n",
    "            weight_n_ab = cp.asarray(weight_n_ab)\n",
    "            energy_list_ab = cp.asarray(self.energy_list_ab)\n",
    "            score_list = cp.empty(0)\n",
    "\n",
    "            for train, test in cv.split(np.arange(n_samples)):\n",
    "                #* Move to GPU\n",
    "                weight_train, energy_train = cp.asarray(weight_n[train]), self.energy_list[train]\n",
    "                weight_test, energy_test = cp.asarray(weight_n[test]), self.energy_list[test]\n",
    "                # print(weight_test.shape)\n",
    "                #* Insert the abnormal dist. \n",
    "                #* Result in: 10% test set + abnormal set.\n",
    "                weight_test = cp.concatenate([weight_test, weight_n_ab], axis=0)\n",
    "                energy_test = cp.concatenate([energy_test, energy_list_ab], axis=0)\n",
    "\n",
    "                regr.fit(weight_train, energy_train)\n",
    "\n",
    "                #* Define the cross-validation score\n",
    "                if fitness_factor == 'CV':\n",
    "                    predict_test = regr.predict(weight_test)\n",
    "                    cv_regr = cp.mean(cp.linalg.norm(predict_test - energy_test))\n",
    "\n",
    "                    score_list = cp.append(score_list, -cv_regr)\n",
    "                \n",
    "                elif fitness_factor == 'R2':\n",
    "                    score_list = cp.append(score_list, regr.score(weight_test, energy_test))\n",
    "\n",
    "            score = np.mean(cp.asnumpy(score_list))\n",
    "            fit_list.append(score)\n",
    "\n",
    "        fit_list = np.array(fit_list)\n",
    "        fit_list_raw = fit_list.copy()\n",
    "        #* Apply a softmax before selecting\n",
    "        fit_list = genetic.softmax(fit_list)\n",
    "        #* Reinforce the prob.?\n",
    "        fit_list = np.power(np.array(fit_list), 1)\n",
    "        #* Probability of survive\n",
    "        ind_h2l = np.argsort(fit_list)[::-1]\n",
    "        #* Weight with the highest score.\n",
    "        p0 = (fit_list/np.sum(fit_list))[ind_h2l[0]]\n",
    "        best_embed = embed_list[ind_h2l[0]]\n",
    "        best_score = fit_list_raw[ind_h2l[0]]\n",
    "\n",
    "        p_list = np.zeros(len(ind_h2l))\n",
    "        for i in range(len(ind_h2l)):\n",
    "            p_list[ind_h2l[i]] = p0*(1-p0)**i\n",
    "        \n",
    "        if len(fit_list) > self.group_size:\n",
    "            ind_survive = np.random.choice(np.arange(len(fit_list)), self.group_size, p=p_list/np.sum(p_list), replace=False)\n",
    "            embed_list = embed_list[ind_survive]\n",
    "            \n",
    "        #* Return the score \n",
    "        return embed_list, np.max(fit_list_raw), np.min(fit_list_raw), best_embed, best_score\n",
    "\n",
    "    @staticmethod\n",
    "    def multi(embed_list, breeding_list, count, re_time):\n",
    "\n",
    "        #* BREEDING(\n",
    "        if count >= len(breeding_list):\n",
    "            breeding_list_chosen = breeding_list[np.random.choice(\n",
    "                np.arange(len(breeding_list)), size=2, replace=False)\n",
    "            ]\n",
    "            embed_list = np.concatenate([ \n",
    "                embed_list, breeding_list_chosen\n",
    "            ], axis=0)\n",
    "\n",
    "        ind_list = np.arange(len(embed_list))\n",
    "        embed_list_n = []\n",
    "\n",
    "        for _ in range(re_time):\n",
    "            \n",
    "            chosen_lists = embed_list[np.random.choice(ind_list, size=2, replace=True)]\n",
    "            chosen_lists[0][-1] = 1\n",
    "            chosen_lists[1][-1] = 1\n",
    "            #* Length of multiplication part\n",
    "            len_multi = np.random.randint(\n",
    "                len(embed_list//2)-len(embed_list//8), len(embed_list//2)+len(embed_list//8)+1\n",
    "            )\n",
    "            # print(chosen_lists)\n",
    "            if _%2 == 0:\n",
    "                chosen_lists[0][len_multi:], chosen_lists[1][len_multi:] = (\n",
    "                    chosen_lists[1][len_multi:], chosen_lists[0][len_multi:])\n",
    "            elif _%2 == 1:\n",
    "                chosen_lists[1][:len_multi], chosen_lists[0][:len_multi] = (\n",
    "                    chosen_lists[0][:len_multi], chosen_lists[1][:len_multi])\n",
    "            embed_list_n += chosen_lists.tolist()\n",
    "\n",
    "        # #* Avoid the 0-list\n",
    "        # nan_ind = np.where(np.linalg.norm(embed_list, axis=1)==0)[0]\n",
    "        # if len(nan_ind) >= 1:\n",
    "        #     for nan in nan_ind:\n",
    "        #         embed_list[nan][0] = 1\n",
    "\n",
    "        #* Return R = re_time*2 X len matrix\n",
    "        return np.unique(np.array(embed_list_n), axis=0)\n",
    "\n",
    "    def mut(self, embed_list):\n",
    "        #* Better to set j at relatively low level\n",
    "        for i in range(len(embed_list)):\n",
    "            for j in range(1): #* Repeat twice for mutation.\n",
    "                if np.random.rand() < self.mut_prob:\n",
    "                    rand_ind = np.random.randint(len(embed_list[0]))\n",
    "                    val = embed_list[i][rand_ind]\n",
    "                    embed_list[i][rand_ind] = np.abs(val-1)\n",
    "                    embed_list[i][-1] = 1\n",
    "\n",
    "        # #* Avoid the 0-list\n",
    "        # nan_ind = np.where(np.linalg.norm(embed_list, axis=1)==0)[0]\n",
    "        # if len(nan_ind) >= 1:\n",
    "        #     for nan in nan_ind:\n",
    "        #         embed_list[nan][0] = 1\n",
    "\n",
    "        return np.unique(np.array(embed_list), axis=0)\n",
    "\n",
    "pth_load_raw = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/ver2_energy_MSAD/runs/20230212/'\n",
    "pth_load = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/'\n",
    "\n",
    "centralize = False\n",
    "per_atom = True\n",
    "#* Load correlation functions\n",
    "weight_pth = pth_load+'weight_50_sym.npy'\n",
    "weight_list = np.load(weight_pth)\n",
    "weight_pth_ab = pth_load+'weight_50_raw_abnormal_sym.npy'\n",
    "weight_list_ab = np.load(weight_pth_ab)\n",
    "\n",
    "#* Standarize features\n",
    "weight_list_all = np.concatenate([weight_list, weight_list_ab], axis=0)\n",
    "\n",
    "#* Load Energy lists\n",
    "energy_pth = pth_load_raw+'energy_list.npy'\n",
    "energy_list = np.load(energy_pth)\n",
    "energy_pth_ab = pth_load_raw+'energy_list_abnormal.npy'\n",
    "energy_list_ab = np.load(energy_pth_ab)\n",
    "\n",
    "if per_atom:\n",
    "    energy_list = energy_list*1e3/32\n",
    "    energy_list_ab = energy_list_ab*1e3/32\n",
    "\n",
    "if centralize:\n",
    "    energy_list = energy_list - np.mean(energy_list)\n",
    "    energy_list_ab = energy_list_ab - np.mean(energy_list_ab)\n",
    "\n",
    "#* Load embed book\n",
    "embed_pth = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/embed_book_new_230212.npy'\n",
    "embed_book = np.load(embed_pth, allow_pickle=True)\n",
    "\n",
    "#* Normalization of weight list.\n",
    "dyna_scalar = False\n",
    "if not dyna_scalar:\n",
    "    scaler = StandardScaler().fit(weight_list_all)\n",
    "    weight_list = scaler.transform(weight_list)\n",
    "    weight_list_ab = scaler.transform(weight_list_ab)\n",
    "\n",
    "#* Hyperparams for GA.\n",
    "trial_step = 1000\n",
    "mut_prob, group_size = 0.06, 30\n",
    "len_cluster = 50\n",
    "re_time_multi = 60\n",
    "len_chosenindividual = 10\n",
    "best_embed, best_score = np.zeros((len_chosenindividual, len_cluster)), -np.ones(len_chosenindividual)*2\n",
    "#* Initialize the genetic algo.\n",
    "gen = genetic(mut_prob, group_size, embed_book, \n",
    "    weight_list, weight_list_ab, energy_list, energy_list_ab,\n",
    "    use_scalar = dyna_scalar)\n",
    "\n",
    "#* Initialize the embedding list, R = 30xlen(cluster).\n",
    "embed_val = np.tile(np.concatenate([np.ones(8), np.zeros(len_cluster-8)]), group_size)\n",
    "np.random.shuffle(embed_val)\n",
    "embed_val = embed_val.reshape(group_size, len_cluster)\n",
    "# embed_val = best_embed_ #* Continue computing.\n",
    "\n",
    "score_h, score_l = [], []\n",
    "count = 0\n",
    "\n",
    "for i in range(trial_step):\n",
    "    \n",
    "    embed_val = np.unique(embed_val, axis=0) #* Remove duplicates.\n",
    "    embed_val = gen.multi(embed_val, best_embed, count, re_time=re_time_multi) #* Multiplication\n",
    "    embed_val = gen.mut(embed_val) #* Mutation\n",
    "    embed_val, max_score, min_score, top_embed, top_score = gen.fitness(\n",
    "        embed_val, fitness_factor='CV') #* Natural Selection\n",
    "\n",
    "    #* Updating the embed list with best performance.\n",
    "    if top_score > np.max(best_score):\n",
    "        best_embed[count%len_chosenindividual] = top_embed\n",
    "        best_score[count%len_chosenindividual] = top_score\n",
    "        count += 1\n",
    "\n",
    "    clear_output(True)\n",
    "    score_h.append(max_score)\n",
    "    score_l.append(min_score)\n",
    "    if i%5 == 0:\n",
    "        x_axis = np.linspace(0, len(score_h)-1, len(score_h))\n",
    "        # plt.ylim(-0.1, 1.1)\n",
    "        plt.plot(x_axis, score_h)\n",
    "        plt.plot(x_axis, score_l)\n",
    "        plt.title(f'CE-E0 Iter: {i}, Max Score: {np.max(score_h)}')\n",
    "        plt.fill_between(x_axis, score_l, score_h, alpha=0.2, color=cm.viridis(0.5))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeking alpha (\n",
    "    \n",
    "for the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(best_embed_)):\n",
    "    #* Extract the weight\n",
    "    false_ind = np.where(best_embed_[i]==0)[0]\n",
    "    delete_ind = np.concatenate(embed_book[false_ind], axis=0)\n",
    "    weight_n = np.delete(weight_list, delete_ind, axis=1)\n",
    "    #* Return the score (fitness)\n",
    "    clf = LassoCV(alphas=np.logspace(-3,3), \n",
    "                max_iter=4000).fit(weight_n, energy_list)\n",
    "    print(clf.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the chosen embedding lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_embed_ = best_embed.copy()\n",
    "# best_embed_ = np.unique(best_embed_, axis=0)\n",
    "print(len(best_embed_), best_embed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_embed_ = best_embed_[0]\n",
    "chosen_embed_1 = chosen_embed_.copy()\n",
    "# chosen_embed_1 = np.load('./runs/demo/20230126_basis_lasso/chosen_embed.npy')\n",
    "# chosen_embed_1[2] = 1.0 #* pair3\n",
    "# chosen_embed_1[26] = 0.0 #* pair5\n",
    "fit_list, fit_list_ab = energy_list, energy_list_ab\n",
    "\n",
    "fit_all = np.concatenate([fit_list, fit_list_ab], axis=0)\n",
    "weight_chosen = genetic.embed_extra(embed_book, weight_list, chosen_embed_1)\n",
    "weight_chosen_ab = genetic.embed_extra(embed_book, weight_list_ab, chosen_embed_1)\n",
    "weight_chosen_all = genetic.embed_extra(embed_book, weight_list_all, chosen_embed_1)\n",
    "\n",
    "# best_embed_dict = dict(zip(embed_type, chosen_embed_1))\n",
    "\n",
    "# dyna_scalar = True #* Use dynamic scalar?\n",
    "#* In case of dynamic scalar, we need to normalize the weight.\n",
    "if dyna_scalar:\n",
    "    scaler = StandardScaler().fit(weight_chosen_all)\n",
    "    weight_chosen = scaler.transform(weight_chosen)\n",
    "\n",
    "clf_ga = LassoCV(alphas=np.logspace(-3,3), cv=10, max_iter=5000).fit(\n",
    "        weight_chosen, fit_list)\n",
    "print(clf_ga.score(weight_chosen, fit_list))\n",
    "# clf_ga_all = RidgeCV(alphas=np.logspace(-3,3), cv=10).fit(weight_chosen_all, msad_all)\n",
    "''' \n",
    "1. Transform the new weights into normalized form. z = (x - u) / s\n",
    "2. Apply the model to new weights, return dist. under that composition.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ga.alpha_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search part. For regressor.\n",
    "\n",
    "A module for testing performance of different regressor.\n",
    "\n",
    "Generally use 3-fold cv, save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_fit(weight_n, energy_list, weight_n_ab, msad_list_ab, regr, mode='LOOCV'):\n",
    "\n",
    "    n_samples = weight_n.shape[0]\n",
    "    count = 0\n",
    "    score_list = []\n",
    "\n",
    "    cv = KFold(n_splits=n_samples)\n",
    "    for train, test in cv.split(np.arange(n_samples)):\n",
    "        weight_train, energy_train = weight_n[train], energy_list[train]\n",
    "        weight_test, energy_test = weight_n[test], energy_list[test]\n",
    "        # print(weight_test.shape)\n",
    "\n",
    "        regr.fit(weight_train, energy_train)\n",
    "        #* 1 + abnormal\n",
    "        if mode == 'abnormalCV':\n",
    "            #* Insert the abnormal dist. \n",
    "            #* Result in: 10% test set + abnormal set.\n",
    "            weight_test = np.concatenate([weight_test, weight_n_ab], axis=0)\n",
    "            msad_test = np.concatenate([msad_test, msad_list_ab], axis=0)\n",
    "            \n",
    "        score_list.append(\n",
    "            np.mean(np.power(regr.predict(weight_test) - energy_test, 2))\n",
    "        )\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "\n",
    "    # score = np.mean(score_list)\n",
    "    return score_list\n",
    "\n",
    "score_list = []\n",
    "# for i in np.linspace(1, 200, 20):\n",
    "#     for j in np.linspace(2, 200, 20):\n",
    "#         for k in range(1,5):\n",
    "#             regr = RandomForestRegressor(max_depth=k, random_state=0, n_jobs=-1,\n",
    "#                 min_samples_leaf=int(i), min_samples_split=int(j))\n",
    "#             score = cv_fit(weight_chosen, msad_list, weight_chosen_ab, msad_list_ab, regr)\n",
    "#             score_list.append([i, j, k, score])\n",
    "\n",
    "score = cv_fit(weight_chosen, energy_list, weight_chosen_ab, energy_list_ab, clf_ga)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = np.array(score_list)\n",
    "sort_indices = np.lexsort((score_list[:, 0], score_list[:, 1], score_list[:,3]))\n",
    "score_list = score_list[sort_indices]\n",
    "print(score_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=4, random_state=0, n_jobs=-1,\n",
    "    min_samples_leaf=int(i), min_samples_split=int(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.fit(weight_chosen, msad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_embed_1 = np.load('./runs/demo/20230126_basis_lasso/chosen_embed.npy')\n",
    "embed_type = [\n",
    "    'pair1', 'pair2', 'pair3', 'pair4', \n",
    "    'tri111', 'tri112', 'tri113', 'tri114',\n",
    "    'tri123', 'tri125', 'tri133', 'tri134',\n",
    "    'tri135', 'tri145', 'tri155', 'tri224',\n",
    "    'tri233', 'tri255', 'tri334', 'tri335',\n",
    "    'tri345', 'tri444', 'tri455',\n",
    "    'qua111111', 'qua111112', 'qua111122',\n",
    "    'pair5', 'pair6',\n",
    "    #? Quadruplets x 21\n",
    "    '111123', '111133', '111134', '111224',\n",
    "    '111233', '111333', '111334', '112233',\n",
    "    '112234', '112333', '112334', '113334',\n",
    "    '113344', '113444', '122334', '123333',\n",
    "    '133444', '222244', '222444', '223333', '223334','point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eci_ind = np.where(np.array(chosen_embed_1) == 1.0)[0]\n",
    "eci_type = [embed_type[i] for i in eci_ind.astype(int)]\n",
    "eci_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pthsav = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230126_basis_lasso/eq_clf_msad_basissym.sav'\n",
    "pickle.dump(clf_ga, open(clf_pthsav, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_embed_pth = './runs/demo/20230126_basis_lasso/chosen_embed.npy'\n",
    "np.save(chosen_embed_pth, chosen_embed_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA implementation, simple but not sufficient for searching the optimal set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Let the temperature decay\n",
    "trial_step = 20000\n",
    "temp_0 = 200\n",
    "temp = temp_0\n",
    "tau = 1200\n",
    "def temp_d(temp_0, t, tau):\n",
    "    return temp_0*np.exp(-t/tau)\n",
    "\n",
    "#* Extract the effective weight according to the embedding list\n",
    "def embed_extra(weight, embed_list, lookbook,):\n",
    "    false_ind = np.where(embed_list==0)[0]\n",
    "    delete_ind = np.concatenate(embed_book[false_ind], axis=0)\n",
    "    weight_ = np.delete(weight, delete_ind, axis=1)\n",
    "    return weight_\n",
    "\n",
    "def random_flip(list_, prob):\n",
    "    if np.random.rand() < prob:\n",
    "        random_ind = np.random.randint(0, len(list_))\n",
    "        val = list_[random_ind]\n",
    "\n",
    "        list_n = list_.copy()\n",
    "        #* Flip the encodes from 0 -> 1 or 1 -> 0\n",
    "        list_n[random_ind] = np.abs(val-1)\n",
    "\n",
    "    else:\n",
    "        list_n = list_\n",
    "\n",
    "    return list_n\n",
    "\n",
    "#* Initialize the embedding list.\n",
    "embed_val = np.zeros(28)\n",
    "weight_list_r = embed_extra(weight_list, embed_val, embed_book)\n",
    "\n",
    "#* Best embedding list corresponding to the highest CV-score.\n",
    "def embed_chosen(list, score, pop_size=10):\n",
    "    len_embed = len(list)\n",
    "    list, score = np.array(list), np.array(score)\n",
    "    if len_embed > pop_size:\n",
    "        #* Get the indices of the highest 10 values\n",
    "        sorted_score = score.argsort()\n",
    "        high10_index = sorted_score[-pop_size:]\n",
    "        list, score = list[high10_index], score[high10_index]\n",
    "    return list.tolist(), score.tolist()\n",
    "\n",
    "chosen_embed = []\n",
    "chosen_score = []\n",
    "best_score = 0\n",
    "\n",
    "len_embed = len(embed_val)\n",
    "score_ = 0\n",
    "score_list = []\n",
    "\n",
    "for i in range(trial_step):\n",
    "    '''\n",
    "    1. Randomly choose one encode to flip.\n",
    "    2. 50% choose another one. \n",
    "    3. 25% choose the 3rd one.\n",
    "    '''\n",
    "    embed_val_ = random_flip(embed_val, prob=1)\n",
    "    embed_val_ = random_flip(embed_val_, prob=1/2)\n",
    "    embed_val_ = random_flip(embed_val_, prob=1/4)\n",
    "\n",
    "    weight_list_n = embed_extra(weight_list, embed_val_, embed_book)\n",
    "    # embed_list = dict(zip(embed_type, embed_val_.astype(bool)))\n",
    "    # msad_list, weight_list = extract_weight(1, 401, embed_list, atom_num=32)\n",
    "\n",
    "    clf = RidgeCV(alphas=np.logspace(-2, 1, 10)).fit(weight_list_n, msad_list.reshape(-1,1))\n",
    "    score = clf.score(weight_list_n, msad_list.reshape(-1,1))\n",
    "\n",
    "    if score >= best_score:\n",
    "        best_score = score.copy()\n",
    "        chosen_embed.append(embed_val_.tolist())\n",
    "        chosen_score.append(score)\n",
    "        #* Return the chosen one(s)\n",
    "        chosen_embed, chosen_score = embed_chosen(chosen_embed, chosen_score)\n",
    "\n",
    "    if np.random.rand()<np.min([1,np.exp((score-score_)/temp)]):\n",
    "        embed_val = embed_val_\n",
    "        score_ = score\n",
    "\n",
    "    #* Temp decay\n",
    "    temp = temp_d(temp_0, i, tau)\n",
    "\n",
    "    clear_output(True)\n",
    "    score_list.append(score)\n",
    "    if i%200 == 0:\n",
    "        plt.plot(score_list)\n",
    "        plt.title(f'Iter: {i}, Max Score: {best_score}, Temp: {temp}')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "for i in range(100):\n",
    "    msad_train, msad_test, weight_train, weight_test = train_test_split(\n",
    "        msad_list.reshape(-1,1), weight_list, test_size=0.05)\n",
    "    \n",
    "    #*Linear regression\n",
    "    weight_train_, msad_train_ = weight_train.clone(), msad_train.clone()\n",
    "    weight_test_, msad_test_ = weight_test.clone(), msad_test.clone()\n",
    "\n",
    "    clf_ = BayesianRidge().fit(weight_train.detach().cpu().numpy(), msad_train.detach().cpu().numpy().flatten())\n",
    "    msad_predict_lr = clf_.predict(weight_test.detach().cpu().numpy())\n",
    "\n",
    "    res = msad_predict_lr-msad_test.numpy().flatten()\n",
    "    res_list += (res.tolist())\n",
    "\n",
    "np.linalg.norm(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msad_train, msad_test, weight_train, weight_test = train_test_split(\n",
    "    msad_list.reshape(-1,1), weight_list, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Linear regression\n",
    "weight_train_, msad_train_ = weight_train, msad_train\n",
    "weight_test_, msad_test_ = weight_test, msad_test\n",
    "\n",
    "#* Bayesian Ridge LR\n",
    "clf_ = BayesianRidge().fit(weight_train_, msad_train_.flatten())\n",
    "msad_predict_lr = clf_.predict(weight_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msad_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Linear regression\n",
    "weight_train_, msad_train_ = weight_train.clone(), msad_train.clone()\n",
    "weight_test_, msad_test_ = weight_test.clone(), msad_test.clone()\n",
    "\n",
    "#* Gaussian Process regression\n",
    "# kernel = DotProduct() + WhiteKernel()\n",
    "kernel = RationalQuadratic()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel,\n",
    "    random_state=0).fit(weight_train_.detach().cpu().numpy(), msad_train_.detach().cpu().numpy().flatten())\n",
    "msad_predict_lr = gpr.predict(weight_test_.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msad_predict_lr-msad_test.numpy().flatten())\n",
    "print(np.linalg.norm(msad_predict_lr-msad_test.numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "date = '20221209'\n",
    "pth_lr = './runs/demo/'\n",
    "create_dir(pth_lr+date)\n",
    "name = '/gpr_msad.sav'\n",
    "pickle.dump(gpr, open(pth_lr+date+name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the current version. (2022.12)\n",
    "\n",
    "To calculate the **TURE** MSAD in randomized states?\n",
    "\n",
    "<i> **Method 1:** </i>\n",
    "\n",
    "1. Generate configs without constraints.\n",
    "\n",
    "2. Put those configs into CE prediction.\n",
    "\n",
    "3. The outputs must follow a distribution that represents the latent dist. at 0 K.\n",
    "\n",
    "Then take **ensemble average** of outputs to obtain $MSAD^{1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_list_predict, weight_list_predict = [], []\n",
    "\n",
    "ce_ = CE(ind_1nn, ind_2nn, ind_3nn, ind_4nn, ind_5nn, ind_6nn,\n",
    "        ind_qua1nn, ind_qua1nn2nn, ind_qua111122,\n",
    "\n",
    "        ind_trip111, ind_trip112, ind_trip113, ind_trip114,\n",
    "        ind_trip123, ind_trip125, ind_trip133, ind_trip134, \n",
    "        ind_trip135, ind_trip145, ind_trip155, ind_trip224,\n",
    "        ind_trip233, ind_trip255, ind_trip334, ind_trip335, \n",
    "        ind_trip345, ind_trip444, ind_trip455, \n",
    "\n",
    "        ind_1nn_pbc, ind_2nn_pbc, ind_3nn_pbc, ind_4nn_pbc, \n",
    "        ind_5nn_pbc, ind_6nn_pbc,\n",
    "        ind_qua1nn_pbc, ind_qua1nn2nn_pbc, ind_qua111122_pbc,\n",
    "\n",
    "        ind_trip111_pbc, ind_trip112_pbc, ind_trip113_pbc, ind_trip114_pbc,\n",
    "        ind_trip123_pbc, ind_trip125_pbc, ind_trip133_pbc, ind_trip134_pbc, \n",
    "        ind_trip135_pbc, ind_trip145_pbc, ind_trip155_pbc, ind_trip224_pbc,\n",
    "        ind_trip233_pbc, ind_trip255_pbc, ind_trip334_pbc, ind_trip335_pbc, \n",
    "        ind_trip345_pbc, ind_trip444_pbc, ind_trip455_pbc, \n",
    "\n",
    "        ind_quaraw, ind_quapbc, ind_raw, use_pbc=True, merge_basis=True)\n",
    "\n",
    "# for i in range(500):\n",
    "#     ele_list = ele_list_gen(0.4, 0.05, 0.4, 0.1, 32)\n",
    "#     ele_list_predict.append(ele_list.tolist())\n",
    "#     weight_list_predict.append((ce_.cluster_extra(\n",
    "#         np.tile(ele_list, 27), embed_list)))\n",
    "''' \n",
    "The configurations without **constraints** on randomness\n",
    "and **normalization** will be generated by \n",
    "</MSAD_predict/weight_extract.py>\n",
    "and stored in <./20221216_msadGA/configs> folder,\n",
    "named by its average composition.\n",
    "'''\n",
    "\n",
    "compo_mat = [\n",
    "        [35, 15, 25],\n",
    "        [40, 5, 27], \n",
    "        [40, 10, 25],\n",
    "        [25, 25, 25]\n",
    "]\n",
    "msad_mat, type_mat = [], []\n",
    "\n",
    "raw_scalar = True\n",
    "dyna_scalar = False\n",
    "assert raw_scalar != dyna_scalar, print('Only one scaler can be used!')\n",
    "\n",
    "load_model = False\n",
    "model_pth = './runs/demo/20230103/eq_clf_msad.sav'\n",
    "\n",
    "for compo in compo_mat:\n",
    "        a1, a2, a3 = compo\n",
    "        atomic_ratio = f'{a1}_{a2}_{a3}_mergebasis.npy'\n",
    "        config = np.load(f'/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools_buffer/msadGA/202212/configs/{atomic_ratio}')\n",
    "\n",
    "        if raw_scalar:\n",
    "                weight_list_predict = scaler.transform(config)\n",
    "        else:\n",
    "                weight_list_predict = config\n",
    "\n",
    "        #* NEW weight matrix remove the zero term.\n",
    "        weight_list_prenew = genetic.embed_extra(embed_book, \n",
    "                                weight_list_predict, \n",
    "                                chosen_embed_1)\n",
    "\n",
    "        #* RAW weight matrix in FULL dimension./ SUB dimension.\n",
    "        if dyna_scalar:\n",
    "                weight_list_prenew = scaler.transform(weight_list_prenew)\n",
    "\n",
    "        if load_model:\n",
    "                clf_ga = pickle.load(open(model_pth, 'rb'))\n",
    "                \n",
    "        msad_list_predict = clf_ga.predict(weight_list_prenew)\n",
    "        print(f'MSAD for Cr{a1}Mn{a2}Co{a3}: {np.mean(msad_list_predict)} pm')\n",
    "        msad_mat.append(msad_list_predict.flatten())\n",
    "        type_mat.append(f'Cr{a1}Mn{a2}Co{a3}')\n",
    "        \n",
    "# print(np.mean(msad_list_predict))\n",
    "for msad_list_predict, type_ in zip(msad_mat, type_mat):\n",
    "        plt.hist(msad_list_predict, bins=50, label=type_, alpha=0.4)\n",
    "        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> **Method 2** </i>\n",
    "\n",
    "Calculate the weight matrix of **randomized** states.\n",
    "\n",
    "Put those weights into linear model and get **precise** prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_ = CE(ind_1nn, ind_2nn, ind_3nn, ind_4nn, ind_5nn, ind_6nn,\n",
    "        ind_qua1nn, ind_qua1nn2nn, ind_qua111122,\n",
    "\n",
    "        ind_trip111, ind_trip112, ind_trip113, ind_trip114,\n",
    "        ind_trip123, ind_trip125, ind_trip133, ind_trip134, \n",
    "        ind_trip135, ind_trip145, ind_trip155, ind_trip224,\n",
    "        ind_trip233, ind_trip255, ind_trip334, ind_trip335, \n",
    "        ind_trip345, ind_trip444, ind_trip455, \n",
    "\n",
    "        ind_1nn_pbc, ind_2nn_pbc, ind_3nn_pbc, ind_4nn_pbc, \n",
    "        ind_5nn_pbc, ind_6nn_pbc,\n",
    "        ind_qua1nn_pbc, ind_qua1nn2nn_pbc, ind_qua111122_pbc,\n",
    "\n",
    "        ind_trip111_pbc, ind_trip112_pbc, ind_trip113_pbc, ind_trip114_pbc,\n",
    "        ind_trip123_pbc, ind_trip125_pbc, ind_trip133_pbc, ind_trip134_pbc, \n",
    "        ind_trip135_pbc, ind_trip145_pbc, ind_trip155_pbc, ind_trip224_pbc,\n",
    "        ind_trip233_pbc, ind_trip255_pbc, ind_trip334_pbc, ind_trip335_pbc, \n",
    "        ind_trip345_pbc, ind_trip444_pbc, ind_trip455_pbc, \n",
    "\n",
    "        ind_quaraw, ind_quapbc, ind_raw, use_pbc=True, merge_basis=True,\n",
    "        normalize_clusterfunc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_ratio = np.array([0.4, 0.4, 0.25, 0.25])\n",
    "\n",
    "#* Scalar part\n",
    "# chosen_embed_ = np.load('./runs/demo/20230103/chosen_embed.npy')\n",
    "pth_embedbook = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20230117_basis_cluster/embed_book_new.npy'\n",
    "embed_book = np.load(pth_embedbook, allow_pickle=True)\n",
    "# embed_book = np.load('./runs/demo/20221216_msadGA/embed_book49.npy')\n",
    "\n",
    "scaler_raw = StandardScaler().fit(weight_list_all)\n",
    "scaler_use = True\n",
    "if scaler_use:\n",
    "    ideal_weight = scaler_raw.transform(ce_.ideal_extract(atomic_ratio).reshape(1,-1))\n",
    "else:\n",
    "    ideal_weight = ce_.ideal_extract(atomic_ratio).reshape(1,-1)\n",
    "\n",
    "ideal_weight_chosen = genetic.embed_extra(embed_book, \n",
    "                                ideal_weight, \n",
    "                                chosen_embed_1)\n",
    "\n",
    "#* Model partfffffffffffffffffffff\n",
    "load_model = False\n",
    "model_pth = './runs/demo/20230103/eq_clf_msad.sav'\n",
    "if load_model:\n",
    "    regr_ga = pickle.load(open(model_pth, 'rb'))\n",
    "else:\n",
    "    regr_ga = clf_ga\n",
    "\n",
    "msad_list_predict = regr_ga.predict(ideal_weight_chosen)\n",
    "msad_list_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ideal_weight_chosen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ce_.ideal_extract(atomic_ratio).reshape(1,-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maybe the last part of the linear model\n",
    "\n",
    "Calculate and draw the $\\partial{MSAD^{1/2}}/\\partial{(A/B)}$ plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_msad(atom_ind1: int, atom_ind2: int):\n",
    "    a1, a2 = embed_type[atom_ind1], embed_type[atom_ind2]\n",
    "    c_1_list = np.linspace(0.01, 0.49, 49)\n",
    "    c_2_list = 0.5 - c_1_list\n",
    "    ratio_list = c_1_list/c_2_list\n",
    "    log_ratiolist = np.log(ratio_list)\n",
    "    msad_predict_list = []\n",
    "\n",
    "    for c_1, c_2 in zip(c_1_list, c_2_list):\n",
    "\n",
    "        atomic_ratio = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "        atomic_ratio[[atom_ind1, atom_ind2]] = c_1, c_2\n",
    "        ideal_weight = scaler_raw.transform(ce_.ideal_extract(atomic_ratio).reshape(1,-1))\n",
    "        ideal_weight_chosen = genetic.embed_extra(embed_book, ideal_weight, chosen_embed_)\n",
    "        msad_predict = regr_ga.predict(ideal_weight_chosen)[0]\n",
    "\n",
    "        msad_predict_list.append(msad_predict)\n",
    "    \n",
    "    return ratio_list, log_ratiolist, msad_predict_list, a1+'/'+a2\n",
    "    \n",
    "# ratio_, msad_, type_ = ratio_msad(0, 1)\n",
    "# type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_ratioresult = {}\n",
    "embed_type = ['Cr', 'Mn', 'Co', 'Ni']\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "count = 1\n",
    "\n",
    "for i, j in combinations(range(4), 2):\n",
    "    print(f'Count: {i, j, count}')\n",
    "    count += 1\n",
    "\n",
    "    ratio_predict, logratio_predict, msad_predict, type_predict = ratio_msad(i, j)\n",
    "    ce_ratioresult[type_predict+'_ratio'] = ratio_predict\n",
    "    ce_ratioresult[type_predict+'_msad'] = msad_predict\n",
    "    ax1.plot(logratio_predict, msad_predict, label='log'+type_predict)\n",
    "    ax2.plot(ratio_predict, msad_predict, label=type_predict)\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "Illustration of ECI-Cluster relationship.\n",
    "'''\n",
    "v_param = regr_ga.coef_\n",
    "v_0 = regr_ga.intercept_\n",
    "len_cum = 0\n",
    "\n",
    "embed_book_chosen = embed_book[eci_ind]\n",
    "cluster_type_chosen = embed_book_chosen.copy()\n",
    "for i in range(len(cluster_type_chosen)):\n",
    "    if i == 8:\n",
    "        j = 4\n",
    "    elif i == 9:\n",
    "        j = 5\n",
    "    elif 3 <= i <= 6:\n",
    "        j = i + 3\n",
    "    elif i == 7:\n",
    "        j = 10\n",
    "    else:\n",
    "        j = i + 1\n",
    "\n",
    "    len_s = len(cluster_type_chosen[i])\n",
    "    len_cum += len_s\n",
    "    print(j, len_cum, len_s)\n",
    "    \n",
    "    cluster_type_chosen[i] = np.ones(len_s)*j\n",
    "\n",
    "cluster_type_chosen = np.concatenate(cluster_type_chosen)\n",
    "plt.plot(cluster_type_chosen, v_param, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_sav = './runs/demo/20230126_basis_lasso/'\n",
    "np.save(pth_sav+'cluster_type_chosen', cluster_type_chosen)\n",
    "np.save(pth_sav+'v_param', v_param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_sav = './runs/demo/20230126_basis_lasso/'\n",
    "\n",
    "# Save the dict\n",
    "with open(f'{pth_sav}ce_ratioresult.pkl', 'wb') as f:\n",
    "    pickle.dump(ce_ratioresult, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dict\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations \n",
    "import matplotlib\n",
    "\n",
    "pth_sav = './runs/demo/20230126_basis_lasso/'\n",
    "with open(f'{pth_sav}ce_ratioresult.pkl', 'rb') as f:\n",
    "    ce_ratioresult = pickle.load(f)\n",
    "\n",
    "embed_type = ['Cr', 'Mn', 'Co', 'Ni']\n",
    "\n",
    "class MyLocator(matplotlib.ticker.AutoMinorLocator):\n",
    "    def __init__(self, n=5):\n",
    "        super().__init__(n=n)\n",
    "matplotlib.ticker.AutoMinorLocator = MyLocator  \n",
    "\n",
    "new_ticksx = np.linspace(-4, 4, 9) \n",
    "new_ticksy = np.linspace(0, 10, 11)#×ø±ê·¶Î§ÉèÖÃ\n",
    "figsize = 30, 30#Í¼Ïñ³ß´ç\n",
    "figure, ax = plt.subplots(figsize = figsize)\n",
    "plt.rcParams[\"xtick.minor.visible\"] =  True\n",
    "plt.rcParams[\"ytick.minor.visible\"] =  True#ÏÔÊ¾´óÐ¡¿Ì¶ÈÏß\n",
    "plt.tick_params(labelsize=23, which = 'major', length = 30, width = 6, direction='in', color = 'k')\n",
    "plt.tick_params(which = 'minor', length = 15, width = 4, direction='in', color = 'k')#¿Ì¶ÈÏß²ÎÊýÉèÖÃ\n",
    "\n",
    "labels = ax.get_xticklabels() + ax.get_yticklabels()#labelÉèÖÃ\n",
    "plt.ylabel(r'$\\mathregular{MSAD^{1/2}\\ (pm)}$', c = 'k', fontsize = 80)\n",
    "#plt.xlabel(r'(screw)', c = 'k', fontproperties = fonttype, fontsize = 100, loc = 'left')\n",
    "plt.xlabel(r'$\\mathregular{log(c_i/c_j)}$', c = 'k', fontsize = 80)#label³ß´çÉèÖÃ\n",
    "plt.xticks(new_ticksx, c = 'k', fontsize = 60)\n",
    "plt.yticks(new_ticksy, c = 'k', fontsize = 60)\n",
    "ax.tick_params(pad = 30)\n",
    "\n",
    "plt.axis([-4, 4, 4, 8])#×ø±êÖáÏÔÊ¾·¶Î§\n",
    "bwith = 10\n",
    "ax.spines['bottom'].set_linewidth(bwith)\n",
    "ax.spines['left'].set_linewidth(bwith)\n",
    "ax.spines['top'].set_linewidth(bwith)\n",
    "ax.spines['right'].set_linewidth(bwith)#ÉèÖÃ×ø±êÖá±ß¿ò\n",
    "\n",
    "for i, j in combinations(range(4), 2):\n",
    "    pair_type = embed_type[i] + '/' + embed_type[j]\n",
    "    ratio_ = np.log(ce_ratioresult[pair_type+'_ratio'])\n",
    "    msad_ = ce_ratioresult[pair_type+'_msad']\n",
    "    plt.plot(ratio_, msad_, label=pair_type, linewidth=7)\n",
    "\n",
    "legend = plt.legend(fontsize=60, edgecolor='black', )\n",
    "legend.set_frame_on(True)\n",
    "legend.get_frame().set_linewidth(7)\n",
    "legend.get_frame().set_edgecolor(\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last last part\n",
    "\n",
    "Find the Pareto optimal value for MSAD and its corresponding composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def single_msad(atom_ratio, regr_ga):\n",
    "    ideal_weight = scaler_raw.transform(ce_.ideal_extract(atomic_ratio).reshape(1,-1))\n",
    "    ideal_weight_chosen = genetic.embed_extra(embed_book, ideal_weight, chosen_embed_)\n",
    "    # msad_predict = regr_ga.predict(ideal_weight_chosen)[0]\n",
    "\n",
    "    return ideal_weight_chosen\n",
    "\n",
    "msad_pareto_list = []\n",
    "atomic_pareto_list = []\n",
    "ideal_weight_pareto_list = []\n",
    "start_ = time.time()\n",
    "count = 0\n",
    "\n",
    "for cr_ in range(1, 51):\n",
    "    # if cr_ % 4 == 0:\n",
    "    #     print(cr_, f'Time consumed: {time.time()-start_}')\n",
    "    for mn_ in range(1, 51-cr_):\n",
    "        # for co_ in range(1, 101-cr_-mn_):\n",
    "        if cr_ % 1 == 0 and mn_ % 1 == 0:\n",
    "            ni_co_ = 100-cr_-mn_\n",
    "            ni_, co_ = ni_co_/2, ni_co_/2\n",
    "            # ni_ = 100-cr_-mn_-co_\n",
    "            # if np.max([co_, ni_]) <= 50:\n",
    "            atomic_ratio = np.array([cr_, mn_, co_, ni_])/100\n",
    "            ideal_weight_chosen = single_msad(atomic_ratio, regr_ga)\n",
    "            ideal_weight_pareto_list.append(\n",
    "                ideal_weight_chosen[0]\n",
    "            )\n",
    "            atomic_pareto_list.append(atomic_ratio.tolist())\n",
    "            \n",
    "            if count % 100 == 0:\n",
    "                print(count, f'Time consumed: {time.time()-start_}')\n",
    "            count += 1\n",
    "\n",
    "msad_pareto_list = regr_ga.predict(np.array(ideal_weight_pareto_list))\n",
    "atomic_pareto_list = np.array(atomic_pareto_list)\n",
    "plt.scatter(atomic_pareto_list[:,0], msad_pareto_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msad_pareto_list = regr_ga.predict(np.array(ideal_weight_pareto_list))\n",
    "atomic_pareto_list = np.array(atomic_pareto_list)\n",
    "plt.scatter(atomic_pareto_list[:,0], msad_pareto_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_pareto_sav = './runs/demo/20230126_basis_lasso'\n",
    "np.save(f'{pth_pareto_sav}/atomic_pareto_list.npy', atomic_pareto_list)\n",
    "np.save(f'{pth_pareto_sav}/msad_pareto_list.npy', msad_pareto_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msad_pareto_list = np.array(msad_pareto_list)\n",
    "cr_pareto_list, mn_pareto_list = atomic_pareto_list[:,0], atomic_pareto_list[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(10, 10))\n",
    "\n",
    "p = ax.scatter(cr_pareto_list*100, mn_pareto_list*100, msad_pareto_list, c = msad_pareto_list, cmap=plt.cm.coolwarm)\n",
    "ax.set_xlabel(r'Cr content %', fontsize=15, rotation=0)\n",
    "ax.set_ylabel(r'Mn content %', fontsize=15, rotation=0)\n",
    "ax.set_zlabel(r'$MSAD^{1/2}\\ (pm)$', fontsize=15, rotation=0)\n",
    "\n",
    "fig.colorbar(p, ax=ax, shrink=0.5, aspect=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The **NN** part\n",
    "\n",
    "Based on the previous implementation of GNN with 32-atom embedding list, just transfer the structure and project it to MSAD value.\n",
    "\n",
    "Trial: Add multiple clusters to $\\widetilde{A}$, by far the 1NN input is not sufficient apparently.\n",
    "\n",
    "Solution for tial: use multichannel input of $\\widetilde{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* FC part\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class fc(nn.Module):\n",
    "    def __init__(self, a_tilt):\n",
    "        super(fc, self).__init__()\n",
    "\n",
    "        self.a_tilt = a_tilt\n",
    "\n",
    "        self.output1 = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.output2 = nn.Linear(32, 1)\n",
    "\n",
    "        self.gcn1 = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gcn2 = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gcn3 = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, atom_list):\n",
    "\n",
    "        input_state = torch.mm(self.a_tilt, atom_list.T).T\n",
    "        s = self.gcn1(input_state)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn2(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.gcn3(s)\n",
    "        s = torch.mm(self.a_tilt, s.T).T\n",
    "        s = self.output2(s)\n",
    "        # print(a_prob)\n",
    "        return s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN-GCN-GCN-1 is verified as the best solution now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_all = np.concatenate([weight_list, weight_list_ab], axis=0)\n",
    "msad_all = np.concatenate([msad_list, msad_list_ab], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Training step\n",
    "mini_batchsize = 16\n",
    "lr_ = 1e-5\n",
    "train_step = 50000\n",
    "epoch_per_episode = 16\n",
    "random_seed = 369\n",
    "\n",
    "date = '20221223msad_GNN'\n",
    "path_save = f'/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/{date}'\n",
    "a_tilt = np.load('/media/wz/7AD631A4D6316195/Projects/SQS_drl/graph/fcc_32/a_tilt.npy')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#* The raw embedding list and corresponding MSAD value\n",
    "pth_ele = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221216_msadGA/ele_list_all.npy'\n",
    "pth_msad = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221216_msadGA/msad_list_all.npy'\n",
    "weight_raw = np.load(pth_ele)\n",
    "msad_list = np.load(pth_msad)\n",
    "\n",
    "mean_msad = np.mean(msad_list)\n",
    "# var_msad = np.var(msad_list)\n",
    "#* Norm.\n",
    "msad_raw = (msad_list.reshape(-1,1)-mean_msad)\n",
    "\n",
    "#* And pass to GPU.\n",
    "weight_raw = torch.from_numpy(weight_raw).to(device).float()\n",
    "msad_raw = torch.from_numpy(msad_raw).to(device).float()\n",
    "a_tilt = torch.from_numpy(a_tilt).to(device).float()\n",
    "\n",
    "#*Device is defined in former block\n",
    "fc_ = fc(a_tilt).to(device)\n",
    "fc_optim = torch.optim.Adam(fc_.parameters(), lr = lr_)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(fc_optim,step_size=10000,gamma = 0.98)\n",
    "mse_loss = nn.MSELoss()\n",
    "# writer = SummaryWriter(log_dir = path_save)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "train_loss, test_loss = [], []\n",
    "#* Divide dataset.\n",
    "weight_train, weight_test, msad_train, msad_test = train_test_split(\n",
    "    weight_raw, msad_raw, train_size=0.8, random_state=random_seed)\n",
    "\n",
    "for i in range(train_step):\n",
    "    # for index in BatchSampler(SubsetRandomSampler(range(len(weight_train))), mini_batchsize, True):\n",
    "        # for epoch in range(epoch_per_episode):\n",
    "\n",
    "    fc_.train()\n",
    "    msad_out_train = fc_(weight_train)\n",
    "    msad_tar_train = msad_train\n",
    "    msad_loss_train = mse_loss(msad_out_train, msad_tar_train)\n",
    "\n",
    "    # writer.add_scalar(\"Training Loss of MSAD\", msad_loss_train, i)\n",
    "    msad_loss_train_ = msad_loss_train\n",
    "    train_loss.append(msad_loss_train_.detach().cpu().numpy().flatten()[0])\n",
    "\n",
    "    fc_optim.zero_grad()\n",
    "    msad_loss_train.backward()\n",
    "    clip_grad_norm_(fc_.parameters(), 0.5)\n",
    "    fc_optim.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    fc_.eval()\n",
    "    msad_out_test = fc_(weight_test)\n",
    "    msad_loss_test = mse_loss(msad_out_test, msad_test)\n",
    "    # writer.add_scalar(\"Testing Loss of MSAD\", msad_loss_test, i)\n",
    "    msad_loss_test_ = msad_loss_test\n",
    "    test_loss.append(msad_loss_test_.detach().cpu().numpy().flatten()[0])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "\n",
    "        plt.plot(train_loss, label='Training loss', alpha=0.6)\n",
    "        plt.plot(test_loss, label='Testing loss', alpha=0.6)\n",
    "\n",
    "        plt.title(f'{np.min(test_loss)}')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fc_.state_dict(), path_save+'/gcn4.pth') #* Save model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate the unknown composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_predict = []\n",
    "\n",
    "# mean_msad = 5.495673167176755\n",
    "# mean_msad = np.mean(msad_list)\n",
    "# date = '20221223msad_GNN'\n",
    "# path_save = f'/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/{date}'\n",
    "# a_tilt = np.load('/media/wz/7AD631A4D6316195/Projects/SQS_drl/graph/fcc_32/a_tilt.npy')\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# a_tilt = torch.from_numpy(a_tilt).float().to(device)\n",
    "\n",
    "# fc_ = fc(a_tilt).to(device)\n",
    "# fc_.load_state_dict(torch.load(path_save+'/gcn4.pth'))\n",
    "fc_.eval()\n",
    "\n",
    "compo_mat = [\n",
    "        [0.25, 0.25, 0.4, 0.1],\n",
    "        [0.4, 0.1, 0.25, 0.25],\n",
    "        [0.25, 0.25, 0.25, 0.25], \n",
    "        [0.1, 0.4, 0.25, 0.25],\n",
    "        [0.25, 0.25, 0.1, 0.4],\n",
    "]\n",
    "for compo in compo_mat:\n",
    "    ele_predict = []\n",
    "    c1, c2, c3, c4 = compo\n",
    "    type_ = f'{int(c1*100)}_{int(c2*100)}_{int(c3*100)}'\n",
    "    for i in range(200000):\n",
    "        ele_list = ele_list_gen(c1, c2, c3, c4, 32)\n",
    "        ele_predict.append(ele_list.tolist())\n",
    "\n",
    "    ele_predict = torch.from_numpy(np.array(ele_predict)).to(device).float()\n",
    "    predict_msad = fc_(ele_predict).detach().cpu().numpy().flatten()+mean_msad\n",
    "\n",
    "    print(f'MSAD for {type_} is {np.mean(predict_msad)}')\n",
    "    plt.hist(predict_msad, bins=100, label=type_, alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_.eval()\n",
    "energy_std = 1\n",
    "energy_mean = 0\n",
    "e_predict = fc_(weight_test).cpu().detach().numpy()\n",
    "e_test_ = msad_test.cpu().detach().numpy()\n",
    "e_predict_lr_ = msad_predict_lr\n",
    "\n",
    "plt.plot(e_predict, label='predict_nn')\n",
    "plt.plot(e_test_, label='test')\n",
    "plt.plot(e_predict_lr_, label='predict_lr')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_res = (e_test_ - e_predict)\n",
    "lr_res = (e_test_ - e_predict_lr_.reshape(-1,1))\n",
    "r_nn_u = np.clip(nn_res, 0, None).flatten()\n",
    "r_nn_l = np.clip(nn_res, None, 0).flatten()\n",
    "r_lr_u = np.clip(lr_res, 0, None).flatten()\n",
    "r_lr_l = np.clip(lr_res, None, 0).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_ylabel('pm/atom')\n",
    "x_axis = np.arange(len(e_test_))\n",
    "plt.scatter(x_axis, nn_res, label='predict_nn_res')\n",
    "plt.scatter(x_axis, lr_res, label='predict_blr_res')\n",
    "plt.plot(x_axis, np.zeros(len(msad_test)), c='k', label='baseline', zorder=0)\n",
    "plt.fill_between(x_axis, r_nn_l, r_nn_u, \n",
    "            alpha=0.2, color=cm.viridis(0.5))\n",
    "plt.fill_between(x_axis, r_lr_l, r_lr_u, \n",
    "            alpha=0.2, color=cm.plasma(0.5))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#* Save NN model\n",
    "pth_save = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221031'\n",
    "torch.save(fc_.state_dict(), pth_save+'/param.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SA processes:\n",
    "\n",
    "1. Starting from **LOCAL MINIMA**\n",
    "\n",
    "2. Start SA, temperature should gradually increases to ensure the stability of system.\n",
    "\n",
    "##### Supplementary\n",
    "\n",
    "1. MC: $$<\\alpha> = \\frac{\\sum\\alpha e^{-\\beta H_i}}{\\sum e^{-\\beta H_i}} \\tag{1}$$\n",
    "\n",
    "MD: $$<\\alpha>_t = 1/t \\sum \\alpha(t) \\tag{2}$$\n",
    "\n",
    "2. $T(t) = T_0e^{-t/\\tau}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Test the setting of temperature decay.\n",
    "def temp(temp_0, t, tau):\n",
    "    return temp_0*np.exp(-t/tau)\n",
    "\n",
    "t = np.linspace(0, 4000, 3001)\n",
    "temp_ = temp(3000, t, 1000)\n",
    "\n",
    "plt.plot(t, temp_)\n",
    "plt.title(f'Final temp.: {temp_[-1]} K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis part for global minimum configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Analysis from .py file\n",
    "import pickle\n",
    "e_list = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221101/min_e_.npy')\n",
    "config_list = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221101/minconfig_.npy')\n",
    "lr_model = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221101/blr.sav'\n",
    "lr_model_ = pickle.load(open(lr_model, 'rb'))\n",
    "\n",
    "# lr_model_.predict(config_list)\n",
    "e_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging console for SRO evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Estimation step\n",
    "#* Use NN?\n",
    "# fc_.eval()\n",
    "\n",
    "def norm_w(x, x_mean, x_std):\n",
    "        return (x - x_mean) / x_std\n",
    "\n",
    "def t_range(temp_0, t, tau):\n",
    "    return temp_0*np.exp(-t/tau)\n",
    "\n",
    "ind_1nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_1nn.npy')\n",
    "ind_2nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_2nn.npy')\n",
    "ind_3nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_3nn.npy')\n",
    "ind_4nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_4nn.npy')\n",
    "ind_qua1nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_qua1nn.npy')\n",
    "ind_qua1nn2nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_qua1nn2nn.npy')\n",
    "ind_trip1nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_trip1nn.npy')\n",
    "ind_trip1nn2nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_trip1nn2nn.npy')\n",
    "ind_trip1nn2nn3nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_trip1nn2nn3nn.npy')\n",
    "\n",
    "ind_1nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_1nn_pbc.npy')\n",
    "ind_2nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_2nn_pbc.npy')\n",
    "ind_3nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_3nn_pbc.npy')\n",
    "ind_4nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_4nn_pbc.npy')\n",
    "ind_qua1nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_qua1nn_pbc.npy')\n",
    "# ind_qua1nn2nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_32/ind_qua1nn2nn_pbc.npy')\n",
    "ind_trip1nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_trip1nn_pbc.npy')\n",
    "ind_trip1nn2nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_trip1nn2nn_pbc.npy')\n",
    "ind_trip1nn2nn3nn_pbc = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_trip1nn2nn3nn_pbc.npy')\n",
    "\n",
    "ind_raw = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_raw2048.npy')\n",
    "\n",
    "ce_e = CE(ind_1nn, ind_2nn, ind_3nn, ind_4nn, \n",
    "        ind_qua1nn, ind_trip1nn, ind_trip1nn2nn, ind_trip1nn2nn3nn,\n",
    "        ind_1nn_pbc, ind_2nn_pbc, ind_3nn_pbc, ind_4nn_pbc, \n",
    "        ind_qua1nn_pbc, ind_trip1nn_pbc, ind_trip1nn2nn_pbc, ind_trip1nn2nn3nn_pbc,\n",
    "        ind_raw)\n",
    "\n",
    "pth_raw = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/'\n",
    "\n",
    "#* Load Classifier\n",
    "blr_12 = pth_raw+'20221107/blr_12.sav'\n",
    "blr_23 = pth_raw+'20221107/blr_23.sav'\n",
    "\n",
    "blr_12 = pickle.load(open(blr_12, 'rb'))\n",
    "blr_23 = pickle.load(open(blr_23, 'rb'))\n",
    "\n",
    "#* Load ECI parameters\n",
    "clf_1 = pth_raw+'20221107/clf_1.sav'\n",
    "clf_2 = pth_raw+'20221107/clf_2.sav'\n",
    "clf_3 = pth_raw+'20221107/clf_3.sav'\n",
    "\n",
    "clf_1 = pickle.load(open(clf_1, 'rb'))\n",
    "clf_2 = pickle.load(open(clf_2, 'rb'))\n",
    "clf_3 = pickle.load(open(clf_3, 'rb'))\n",
    "\n",
    "#* Load tensorboard\n",
    "date = '20221113'\n",
    "pth_tb = f'/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/{date}'\n",
    "writer = SummaryWriter(log_dir = pth_tb)\n",
    "\n",
    "energy_std, energy_mean = 1, 0\n",
    "\n",
    "config = ele_list_gen(1/4, 1/4, 1/4, 1/4, num_c=2048)\n",
    "iter_time= 10000\n",
    "temp = 1200\n",
    "k_ = 8.617333262e-5\n",
    "atom_num = 2048\n",
    "# energy_std = 1\n",
    "config_list = np.zeros((50, atom_num))\n",
    "e_list_store = np.zeros(50)\n",
    "sro_list_store = np.zeros((iter_time, 12))\n",
    "\n",
    "for i in range(iter_time):\n",
    "        # weight_config = norm_w(ce_e.cluster_extra(config).reshape(-1,1).T,\n",
    "        #                 weight_mean, weight_std)\n",
    "        config_tile = np.tile(config, 27) #* In 3*3*3 PBC space.\n",
    "        weight_config  = ce_e.cluster_extra(config_tile).reshape(-1,1).T\n",
    "\n",
    "        #* Prediction part\n",
    "        label = int(blr_12.predict(weight_config)[0])\n",
    "        if label == 0:\n",
    "                energy = (clf_1.predict(weight_config)*energy_std + energy_mean)[0]\n",
    "        else:\n",
    "                label_ = int(blr_23.predict(weight_config)[0])\n",
    "                if label_ == 0:\n",
    "                        energy = (clf_2.predict(weight_config)*energy_std + energy_mean)[0]\n",
    "                else:\n",
    "                        energy = (clf_3.predict(weight_config)*energy_std + energy_mean)[0]  \n",
    "\n",
    "        #* NN's prediction\n",
    "        # weight_config = torch.from_numpy(weight_config.astype(np.float32)).clone().to(device)\n",
    "        # energy = fc_(weight_config).cpu().detach().numpy()[0,0]*energy_std + energy_mean\n",
    "        energy *= atom_num\n",
    "        # e_list.append(energy)\n",
    "        config_list[i%50] = config\n",
    "        e_list_store[i%50] = energy\n",
    "        #* Extract SRO params of current config..\n",
    "        sro = sub_func_ce.sro_extra(ind_1nn, config, 0.25, 0.25, 0.25, 0.25)\n",
    "        sro_list_store[i%iter_time] = sro\n",
    "        \n",
    "        writer.add_scalar(f'Thermo-MC Energy (eV)', energy, i)\n",
    "        writer.add_scalar(f'SRO param/CrCr', sro[0], i)\n",
    "        writer.add_scalar(f'SRO param/MnMn', sro[1], i)\n",
    "        writer.add_scalar(f'SRO param/CoCo', sro[2], i)\n",
    "        writer.add_scalar(f'SRO param/NiNi', sro[3], i)\n",
    "        writer.add_scalar(f'SRO param/CrMn', sro[4], i)\n",
    "        writer.add_scalar(f'SRO param/CrCo', sro[5], i)\n",
    "        writer.add_scalar(f'SRO param/CrNi', sro[6], i)\n",
    "        writer.add_scalar(f'SRO param/MnCo', sro[7], i)\n",
    "        writer.add_scalar(f'SRO param/MnNi', sro[8], i)\n",
    "        writer.add_scalar(f'SRO param/CoNi', sro[9], i)\n",
    "\n",
    "        while True:\n",
    "                a_ind = randrange(len(ind_1nn))\n",
    "                action = ind_1nn[a_ind]\n",
    "                a1, a2 = config[action[0]], config[action[1]]\n",
    "                if a1 != a2:\n",
    "                        break\n",
    "\n",
    "        config_ = swap_step(action, config)\n",
    "\n",
    "        assert np.linalg.norm(np.sort(config_)-np.sort(config)) == 0, print(f'BUG in swap_step')\n",
    "        # weight_config_ = norm_w(ce_e.cluster_extra(config_).reshape(-1,1).T,\n",
    "        #                 weight_mean, weight_std)\n",
    "        config_tile_ = np.tile(config_, 27) #* In 3*3*3 PBC space.\n",
    "        weight_config_ = ce_e.cluster_extra(config_tile_).reshape(-1,1).T\n",
    "        #* NN's prediction\n",
    "        # weight_config_ = torch.from_numpy(weight_config_.astype(np.float32)).clone().to(device)\n",
    "        # energy_ = fc_(weight_config_).cpu().detach().numpy()[0,0]*energy_std + energy_mean\n",
    "        #* Prediction part\n",
    "        label = int(blr_12.predict(weight_config_)[0])\n",
    "        if label == 0:\n",
    "                energy_ = (clf_1.predict(weight_config_)*energy_std + energy_mean)[0]\n",
    "        else:\n",
    "                label_ = int(blr_23.predict(weight_config_)[0])\n",
    "                if label_ == 0:\n",
    "                        energy_ = (clf_2.predict(weight_config_)*energy_std + energy_mean)[0]\n",
    "                else:\n",
    "                        energy_ = (clf_3.predict(weight_config_)*energy_std + energy_mean)[0] \n",
    "\n",
    "        energy_ *= atom_num\n",
    "\n",
    "        accept = np.min([1, np.exp((energy-energy_)/(k_*temp))])\n",
    "        r_v = np.random.rand()\n",
    "        if r_v <= accept:\n",
    "                config = config_\n",
    "        else:\n",
    "                config = config\n",
    "\n",
    "        clear_output(True)\n",
    "        # plt.title(f'Iter num {i} at {temp} K')\n",
    "        # plt.plot(e_list)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ele_list_gen(1/4, 1/4, 1/4, 1/4, num_c=2048)\n",
    "ind_1nn = np.load('/media/wz/7AD631A4D6316195/Projects/mc_pure_qua/fcc_2048/ind_1nn.npy')\n",
    "sub_func_ce.sro_extra(ind_1nn, config, 0.25, 0.25, 0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from fitter import Fitter\n",
    "#* Check the distribution of energy\n",
    "for i in range(sro_list_store.shape[1]-2):\n",
    "    sro = sro_list_store[:,i]\n",
    "    plt.hist(sro, bins=100, alpha=0.4)\n",
    "\n",
    "plt.show()\n",
    "# f_pair = Fitter(mncr_list, distributions = ['norm'], timeout = 1500)\n",
    "# f_pair.fit()\n",
    "\n",
    "# print(f_pair.get_best())\n",
    "# f_pair.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221107/sro_1200.npy', sro_list_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sro_100 = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221107/sro_100.npy')\n",
    "sro_400 = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221107/sro_400.npy')\n",
    "sro_1200 = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221107/sro_1200.npy')\n",
    "# sro_4000 = np.load('/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221107/sro_4000.npy')\n",
    "\n",
    "# sro_100_mean = np.mean(sro_100[1000:], 0).reshape(-1,1)\n",
    "sro_400_mean = np.mean(sro_400[1000:], 0).reshape(-1,1)\n",
    "sro_1200_mean = np.mean(sro_1200[1000:], 0).reshape(-1,1)\n",
    "# sro_4000_mean = np.mean(sro_4000[1000:], 0).reshape(-1,1)\n",
    "\n",
    "sro_mean = np.concatenate([sro_400_mean, sro_1200_mean], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 15))\n",
    "\n",
    "for i in range(len(sro_400_mean)-2):\n",
    "    plt.plot(sro_mean[i])\n",
    "\n",
    "plt.ylim([-0.1, 0.6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing CE estimations\n",
    "\n",
    "Results are following an order of \n",
    "\n",
    "a_crcr, a_mnmn, a_coco, a_nini, a_mncr, a_cocr, a_nicr, a_comn, a_nimn, a_nico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sro_name = ['a_crcr', 'a_mnmn', 'a_coco', 'a_nini', 'a_mncr', 'a_cocr', 'a_nicr', 'a_comn', 'a_nimn', 'a_nico']\n",
    "temp_range = np.array([100, 200, 300, 400, 500, 800, 1000, 1200, 1600, 2400, 3200, 4800])\n",
    "sro_list = np.zeros((len(temp_range), 10)) #* Nx10\n",
    "pth_raw = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221101/'\n",
    "for i in range(len(temp_range)):\n",
    "    sro_list[i] = np.mean(np.load(pth_raw+f'sro_{temp_range[i]}.npy'), axis=0)\n",
    "\n",
    "sro_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(temp_range, sro_list[:, i], label=sro_name[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "499e11cead59c3d3cc67ca76f44a2d3f924d0c922327208cb4a57cb2c720af19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
