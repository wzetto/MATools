{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* FC part\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class fc(nn.Module):\n",
    "    def __init__(self, a_tilts):\n",
    "        super(fc, self).__init__()\n",
    "\n",
    "        self.a_tilts = a_tilts\n",
    "        self.channels1 = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "        ) for _ in range(len(a_tilts))])\n",
    "\n",
    "        self.channels2 = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "        ) for _ in range(len(a_tilts))])\n",
    "\n",
    "        self.output1 = nn.Sequential(\n",
    "            nn.Linear(len(a_tilts) * 32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.output2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, atom_list):\n",
    "        \n",
    "        input_state = torch.mm(self.a_tilts[0], atom_list.T).T\n",
    "        channels = self.channels1[0](input_state)\n",
    "        channels = torch.mm(self.a_tilts[0], channels.T).T\n",
    "        channels = self.channels2[0](channels)\n",
    "\n",
    "        for i in range(1, len(self.a_tilts)):\n",
    "            input_state = torch.mm(self.a_tilts[i], atom_list.T).T\n",
    "            channels = torch.cat((channels, self.channels1[i](input_state)), dim=1)\n",
    "\n",
    "        print(channels.shape)\n",
    "        input_state = torch.mm(self.a_tilts[0], channels[0]).T\n",
    "        channels_ = self.channels2[0](input_state)\n",
    "        for i in range(1, len(self.a_tilts)):\n",
    "            input_state = torch.mm(self.a_tilts[i], channels[i]).T\n",
    "            channels_ = torch.cat((channels_, self.channels2[i](input_state)), dim=1)\n",
    "\n",
    "        channels_ = torch.cat(channels_, dim=1).flatten()\n",
    "        s = self.output1(channels_)\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 192])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat2 must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4047102/553989872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mfc_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmsad_out_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mmsad_tar_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsad_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mmsad_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsad_out_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsad_tar_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4047102/1945935225.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, atom_list)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0minput_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_tilts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mchannels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_tilts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat2 must be a matrix"
     ]
    }
   ],
   "source": [
    "#* Training step\n",
    "mini_batchsize = 16\n",
    "lr_ = 1e-5\n",
    "train_step = 50000\n",
    "epoch_per_episode = 16\n",
    "random_seed = 369\n",
    "\n",
    "date = '20230211msad_GNN'\n",
    "path_save = f'./runs/{date}'\n",
    "a_tilt = np.array([\n",
    "    np.load(f'./laplacian/d_{i}nn.npy') for i in range(1, 7)\n",
    "])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#* The raw embedding list and corresponding MSAD value\n",
    "pth_ele = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221216_msadGA/ele_list_all.npy'\n",
    "pth_msad = '/media/wz/a7ee6d50-691d-431a-8efb-b93adc04896d/Github/MATools/CE_MC/runs/demo/20221216_msadGA/msad_list_all.npy'\n",
    "weight_raw = np.load(pth_ele)\n",
    "msad_list = np.load(pth_msad)\n",
    "\n",
    "mean_msad = np.mean(msad_list)\n",
    "# var_msad = np.var(msad_list)\n",
    "#* Norm.\n",
    "msad_raw = (msad_list.reshape(-1,1)-mean_msad)\n",
    "\n",
    "#* And pass to GPU.\n",
    "weight_raw = torch.from_numpy(weight_raw).to(device).float()\n",
    "msad_raw = torch.from_numpy(msad_raw).to(device).float()\n",
    "a_tilt = torch.from_numpy(a_tilt).to(device).float()\n",
    "\n",
    "#*Device is defined in former block\n",
    "fc_ = fc(a_tilt).to(device)\n",
    "fc_optim = torch.optim.Adam(fc_.parameters(), lr = lr_)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(fc_optim,step_size=10000,gamma = 0.98)\n",
    "mse_loss = nn.MSELoss()\n",
    "# writer = SummaryWriter(log_dir = path_save)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "train_loss, test_loss = [], []\n",
    "#* Divide dataset.\n",
    "weight_train, weight_test, msad_train, msad_test = train_test_split(\n",
    "    weight_raw, msad_raw, train_size=0.8, random_state=random_seed)\n",
    "\n",
    "for i in range(train_step):\n",
    "    # for index in BatchSampler(SubsetRandomSampler(range(len(weight_train))), mini_batchsize, True):\n",
    "        # for epoch in range(epoch_per_episode):\n",
    "\n",
    "    fc_.train()\n",
    "    msad_out_train = fc_(weight_train)\n",
    "    msad_tar_train = msad_train\n",
    "    msad_loss_train = mse_loss(msad_out_train, msad_tar_train)\n",
    "\n",
    "    # writer.add_scalar(\"Training Loss of MSAD\", msad_loss_train, i)\n",
    "    msad_loss_train_ = msad_loss_train\n",
    "    train_loss.append(msad_loss_train_.detach().cpu().numpy().flatten()[0])\n",
    "\n",
    "    fc_optim.zero_grad()\n",
    "    msad_loss_train.backward()\n",
    "    clip_grad_norm_(fc_.parameters(), 0.5)\n",
    "    fc_optim.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    fc_.eval()\n",
    "    msad_out_test = fc_(weight_test)\n",
    "    msad_loss_test = mse_loss(msad_out_test, msad_test)\n",
    "    # writer.add_scalar(\"Testing Loss of MSAD\", msad_loss_test, i)\n",
    "    msad_loss_test_ = msad_loss_test\n",
    "    test_loss.append(msad_loss_test_.detach().cpu().numpy().flatten()[0])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "\n",
    "        plt.plot(train_loss, label='Training loss', alpha=0.6)\n",
    "        plt.plot(test_loss, label='Testing loss', alpha=0.6)\n",
    "\n",
    "        plt.title(f'{np.min(test_loss)}')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68475d8e8ba7c27bff5b0c1dcce162ecdafd8f583568d2d03f898fe272d0ccc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
